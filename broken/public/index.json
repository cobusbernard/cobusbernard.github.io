[{"categories":["gen-ai"],"contents":"I\u0026rsquo;ve been neglecting my blog, and decided it is time to start writing again. So like any good software developer, I don\u0026rsquo;t actually write a post, I start by tinkering with my blog setup. I initially ran it as a Jekyll generated static site on GitHub using GitHub Pages, and then as part of a live-stream, moved it to use AWS Amplify to host it in on of my many AWS accounts. Coming back to it a few years later, I still remember the headaches of dealing with different Ruby versions, getting rvm set up, and then only getting it going. The paging is currently not working, so I\u0026rsquo;ll be trying to move over to Hugo for this.\nThe first step was to find a theme I liked, settled on Toha by Emruz Hossain specifically for how I can have multiple sub-sections and notes. When looking at the GitHub repo, it starts with a note about using hugo modules instead of the git submodule approach I previously used. Love how this yak-shaving mimics my usually day-to-day work where I only want to do that 1 things, but end up going down the rabbit hole. I\u0026rsquo;m jotting down the steps as I go for future reference.\nGetting some help from Amazon Q Since I have no idea about what the tradeoffs are, I decided to use Amazon Q to help me through this whole process. I installed the Amazon Q extension in VSCode, authenticated, and started asking away. Step one is to ask it:\nPrompt: ======= When setting up a new site using Hugo, can you please explain the difference between a git submodule vs a go module w.r.t. installing a theme? Response: ========= Certainly, I\u0026#39;d be happy to explain the difference between using a Git submodule versus a Go module when installing a theme for a new Hugo site. When using a Git submodule, you essentially link your Hugo site\u0026#39;s repository to the repository of the theme you want to use. This means that the theme\u0026#39;s source code is maintained in a separate repository, and your Hugo site\u0026#39;s repository contains a reference to that theme repository. This approach has a few advantages: [1] 1. Separation of Concerns: By keeping the theme in a separate repository, you can more easily update the theme independently of your site\u0026#39;s content and configuration. 2. Versioning: You can specify a specific version or commit of the theme that you want to use, which helps with maintaining consistency across different deployments of your site. 3. Collaboration: If you\u0026#39;re working on a team, using a Git submodule makes it easier for other team members to clone your site\u0026#39;s repository and have the theme available as well. On the other hand, using a Go module for a theme integration is a more modern approach that is recommended by the Hugo team. With Go modules, the theme\u0026#39;s source code is directly included in your Hugo site\u0026#39;s repository, and the theme is treated as a dependency that is fetched and compiled as part of the build process. The main advantages of using Go modules are: 1. Simplicity: There\u0026#39;s no need to manage the submodule separately, as the theme is just another dependency that is handled by the Go module system. 2. Reproducibility: When you build your Hugo site, the exact versions of the dependencies (including the theme) are guaranteed to be used, making the build process more reliable and reproducible. 3. Tooling Integration: Go modules integrate well with the standard Go tooling, which can make managing dependencies and updating themes easier. In summary, while both approaches can be used to install a theme for a Hugo site, the Go module approach is generally considered the more modern and recommended way, as it provides a more streamlined and integrated dependency management experience. Sources [1] Hugo website on github pages deployed through github actions is not working with custom theme stackoverflow.comquestions67725758 Planning Before we get too carried away, let\u0026rsquo;s build ourselves a todo list to move the blog:\nPrompt: ======= I have a personal blog currently deployed using AWS Amplify from a GitHub repository that is public, with Disqus for comments, Google Analytics for my site analytics, and would like to change it to use Hugo, but keep the other technologies the same, can you please create a todo list for me to ensure I don\u0026#39;t forget to migrate something? Response: ========= Certainly, here\u0026#39;s a suggested todo list to help you migrate your personal blog from GitHub Pages with Jekyll to using Hugo, while keeping the existing technologies like AWS Amplify, Disqus, and Google Analytics: TODO List for Migrating Personal Blog from Jekyll to Hugo: 1. Set up a new Hugo project - Create a new Hugo site using the `hugo new site` command - Choose a suitable Hugo theme, such as the Toha theme you mentioned previously 2. Migrate content from Jekyll to Hugo - Convert your existing Markdown posts to the Hugo format - Ensure that the front matter (metadata) in your posts is compatible with Hugo 3. Configure AWS Amplify for the new Hugo site - Create a new Amplify app for your Hugo site - Set up the necessary build settings and environment variables - Configure the Amplify app to use your new Hugo site\u0026#39;s repository 4. Integrate Disqus comments - Add the necessary Disqus shortname and configuration to your Hugo site\u0026#39;s templates 5. Integrate Google Analytics - Add the Google Analytics tracking code to your Hugo site\u0026#39;s templates - Ensure that the tracking code is compatible with the latest version of Google Analytics (GA4) 6. Update DNS and domain settings - If you\u0026#39;re using a custom domain, update the DNS settings to point to your new Hugo site hosted on AWS Amplify 7. Test the migration - Thoroughly test your new Hugo-based site to ensure that all features, content, and integrations are working as expected - Verify that your site is rendering correctly and that all links, images, and other assets are properly loaded 8. Redirect old Jekyll-based URLs (optional) - If you had a significant number of existing URLs for your Jekyll-based site, consider setting up URL redirects to ensure that existing links continue to work 9. Update any external references (e.g., social media, email signatures) - Update any references to your old Jekyll-based site with the new Hugo-based site URL 10. Monitor and maintain the new Hugo-based site - Set up monitoring and alerting for your new Hugo-based site hosted on AWS Amplify - Keep your Hugo version, theme, and dependencies up-to-date to ensure security and stability Remember, this is a general todo list, and the specific steps may vary depending on your existing setup and the technologies you\u0026#39;re using. It\u0026#39;s always a good idea to thoroughly test and validate each step of the migration process to ensure a smooth transition for your personal blog. The toha theme mentioned above is due to a longer discussion on figuring out the theme install first, but this is a decent list. The only step I would update is point 4 - for Disqus, you would also need to migrate the existing comments if the domain or path structure of the content changed. Luckily there are a few ways to do it, I will just need to look into how to handle redirects with Hugo, it should be a standard feature.\nI do need to confess that I went down a rabbit hole first here when I followed the quickstart without realising those instructions are for modifying and publishing the theme to then pull into Hugo as a module. But I digress, let\u0026rsquo;s build this!\nBarebones site Following the steps in the getting started guide, I ran the following:\ngit checkout -b switch-to-hugo # Let\u0026#39;s not break the site before we are ready to move mkdir old mv !(old)* old/ # Include all files / folders, but not hidden ones, and exclude the one called \u0026#34;old\u0026#34;. hugo new site ./ --format=yaml --force hugo mod init github.com/cobusbernard/cobusbernard.github.io # Repo name is a left-over from when I used GitHub pages # Adding the theme module to the Hugo config cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; hugo.yaml module: imports: - path: github.com/hugo-toha/toha/v4 mounts: - source: ./node_modules/flag-icon-css/flags target: static/flags - source: ./node_modules/@fontsource/mulish/files target: static/files - source: ./node_modules/katex/dist/fonts target: static/fonts EOF hugo mod get -u hugo mod tidy hugo mod npm pack npm install hugo server -w And voila! The site is up and running, but very bare. I then spent some time updating the configs with all my details, and added the sections I needed. Next, I need to migrate my old content from the Jekyll front matter format to the one for Hugo. This was a tedious, manual task, but an hour or 3 later I was done. Also, past-Cobus really needs to sort out consistency in filenames, this is what I found - you can clearly spot when I started working at a company that used quite a lot of Python:\n. ├── 2011-07-25-Ubuntu-on-HyperV.md ├── 2012-02-04-git-rpc-error.md ├── 2012-03-27-Terminal-install-Java-Ubuntu-10.10.md ├── 2012-04-10-chrome_default_xubuntu.md ├── 2012-04-12-ssh-config-multiple-keys.md ├── 2013-01-28-ssh-rate-limit.md ├── 2015-01-26-basic-github-jekyll-blog.md ├── 2015-02-08-convert-putty-keys-to-openssh.md ├── 2015-02-13-openvpn-on-digital-ocean.md ├── 2015-02-14-openvpn-home.md ├── 2015-02-17-route-53-dns-update-script.md ├── 2015-02-19-mdadm-replace-drive.md ├── 2015-02-27-git-checkout-ssh-vs-https.md ├── 2015-03-30-MSBuild-External-Error.md ├── 2015-04-04-GitFlow-SemVer-PSake.md ├── 2015-12-16-NTP-on-AWS.md ├── 2016-02-06-Parameterizing_Webconfig.md ├── 2016-02-18-Upgrading_Berks_Dependency_Issue.md ├── 2016-06-12-Centos_Gluster_Startup.md ├── 2016-09-03-AWS_Multi_Account.md ├── 2017-01-15-timemachine-backups.md ├── 2017-02-05-Yak_shaving_Makefiles.md ├── 2017-02-09-OSX_Home_End_Keys.md ├── 2024-05-28-migrating-jekyll-to-hugo.md └── _index.md Deploying the new version Now that the main site is ready, I want to deploy it from the branch to double check that everything works and I didn\u0026rsquo;t miss anything. This is fairly simple with Amplify,\nMigrating Disqus comments Now that I have the content migrated and redirects in place via the following in the front matter for each post:\naliases: - /version%20control/2012/02/04/git-rpc-error It is time to migrate the existing comments to the new URLs. I will be using the automatic update that relies on 301 redirects being in place - I went through each post to confirm the redirect is correctly in place.\n","date":"May 28, 2024","hero":"/posts/2024-05-28/migrating-jekyll-to-hugo/images/amazon_q_migrate_jekyll_to_hugo.png","permalink":"http://localhost:1313/posts/2024-05-28/migrating-jekyll-to-hugo/","summary":"I\u0026rsquo;ve been neglecting my blog, and decided it is time to start writing again. So like any good software developer, I don\u0026rsquo;t actually write a post, I start by tinkering with my blog setup. I initially ran it as a Jekyll generated static site on GitHub using GitHub Pages, and then as part of a live-stream, moved it to use AWS Amplify to host it in on of my many AWS accounts.","tags":["linux","aws-amplify","amazon-q","hugo","blog"],"title":"Migrating my Jekyll blog to Hugo with Gen-AI"},{"categories":["OSX"],"contents":"The Problem As I was pasting something into Chrome this morning, I, once again, expected the Home key to take me to the start of the URL. It. Did. Not. So I decided that I won\u0026rsquo;t be able to change this ingrained behaviour and need to fix it. First Google hit led me to a post by Damian Guard that I will be pasting below to keep as notes for myself. Everything from here on is from his post.\nMake Home \u0026amp; End keys behave like Windows on Mac OS X If, like me, you want Home to send you to the start of the line and not to the top of the document then create a file called DefaultKeyBinding.dict in your ~/Library/KeyBindings folder (might need to create that folder too) with the following contents:\n{ \u0026#34;\\UF729\u0026#34; = moveToBeginningOfParagraph:; // home \u0026#34;\\UF72B\u0026#34; = moveToEndOfParagraph:; // end \u0026#34;$\\UF729\u0026#34; = moveToBeginningOfParagraphAndModifySelection:; // shift-home \u0026#34;$\\UF72B\u0026#34; = moveToEndOfParagraphAndModifySelection:; // shift-end \u0026#34;^\\UF729\u0026#34; = moveToBeginningOfDocument:; // ctrl-home \u0026#34;^\\UF72B\u0026#34; = moveToEndOfDocument:; // ctrl-end \u0026#34;^$\\UF729\u0026#34; = moveToBeginningOfDocumentAndModifySelection:; // ctrl-shift-home \u0026#34;^$\\UF72B\u0026#34; = moveToEndOfDocumentAndModifySelection:; // ctrl-shift-end } This remapping does the following in most Mac apps including Chrome (some apps do their own key handling):\nHome and End will go to start and end of line Shift+Home and Shift+End will select to start and end of line Ctrl+Home and Ctrl+End will go to start and end of document Shift+Ctrl+Home and Shift+Ctrl+End will select to start and end of document ","date":"February 9, 2017","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2017-02-09/osx-home-end-keys/","summary":"The Problem As I was pasting something into Chrome this morning, I, once again, expected the Home key to take me to the start of the URL. It. Did. Not. So I decided that I won\u0026rsquo;t be able to change this ingrained behaviour and need to fix it. First Google hit led me to a post by Damian Guard that I will be pasting below to keep as notes for myself.","tags":["osx"],"title":"Remapping the Home and End keys on OSX"},{"categories":["Linux"],"contents":"The Problem I woke up this morning wanting to start on a long post about Terraform and how I ended up with the structure shown in my repo. I recently reinstalled my Macbook using Netatalk for a timemachine , and as I fired up my vagrant image, it started to pull down the Ubuntu box first and then started configuring it. This is very useful, but it is slow. And it broke. I forgot to add -y to the apt-get when I initially created the install script:\n==\u0026gt; jekyll: Do you want to continue? ==\u0026gt; jekyll: [Y/n] ==\u0026gt; jekyll: Abort. I have been working with docker a lot at work and while running a 3 node Mesos + Marthon cluster at home (future post +1). Inspired by the DevOpsDays site, I wanted to have my own container to work with when writing blog posts. (And convince myself that doing this will make me more efficient and therefore create more blog entries\u0026hellip;). While I\u0026rsquo;m at it, I wanted to use a Makefile. I have encountered them before, but never created one, so I decided on the following Yaks to shave:\nCreate and use a container to run the site Build the container using a Makefiles (Found later) Upgrade to Ruby 2.2.5 Creating the container via Makefile First hit in Google for getting started with makefiles yielded this site. Seems fairly straightforward. I always want some default behaviour in a command that doesn\u0026rsquo;t break anything and explains what it does when you run it without any parameters. To do so, I created the first target as default with a nice little message describing the available targets:\ndefault: echo \u0026#34;This file is used to work with the cobus.io Jekyll blog site.\u0026#34; echo \u0026#34;The following commands are available:\u0026#34; echo \u0026#34; - docker : builds the docker container to run the site locally.\u0026#34; echo \u0026#34; - run : runs the docker container with the site\u0026#34; Running make outputs the following:\n~/Projects/Sites/cobus.io git:(master) ✗ 17-02-05 8:25 make echo \u0026#34;This file is used to work with the cobus.io Jekyll blog site.\u0026#34; This file is used to work with the cobus.io Jekyll blog site. echo \u0026#34;The following commands are available:\u0026#34; The following commands are available: echo \u0026#34; - docker : builds the docker container to run the site locally.\u0026#34; - docker : builds the docker container to run the site locally. echo \u0026#34; - run : runs the docker container with the site\u0026#34; - run : runs the docker container with the site Damn, that is ugly. So how do I hide the command ala echo off? Google! Prefix it with an @. Update 2017-02-08: Geoff Kruss pointed out on ZA Tech Slack that this is considered bad practice as you can hide the commands by using make -s for silent mode. Updating the Makefile references to only have the echo commands suppressed.\ndefault: @echo \u0026#34;This file is used to work with the cobus.io Jekyll blog site.\u0026#34; @echo \u0026#34;The following commands are available:\u0026#34; @echo \u0026#34; - docker : builds the docker container to run the site locally.\u0026#34; @echo \u0026#34; - run : runs the docker container with the site\u0026#34; This looks a lot better:\n➜ ~/Projects/Sites/cobus.io git:(master) ✗ 17-02-05 8:25 make This file is used to work with the cobus.io Jekyll blog site. The following commands are available: - docker : builds the docker container to run the site locally. - run : runs the docker container with the site I started with the docker target to create the container to be able to see if my ramblings here render correctly. Additionally, I want to enforce a version number when building the container (I love version numbers). It will allow pegging which version of the container to use when running this blog as well as check out historic points in time and still be able to run the blog. I found this answer for detecting missing variables and went for the multi-line option:\ndocker: ifeq ($(VERSION),) @echo \u0026#34;Please set the VERSION environment variable before building the continer.\u0026#34; exit 1 endif @echo \u0026#34;Preparing to build version [${VERSION}] of cobusbernard/jekyll-blog container...\u0026#34; Which didn\u0026rsquo;t work:\nifeq (,) /bin/sh: -c: line 0: syntax error near unexpected token `,\u0026#39; /bin/sh: -c: line 0: `ifeq (,)\u0026#39; make: *** [docker] Error 2 That looks like it needs me to specify bash usage, so I added SHELL:=/bin/bash at the top of the Makefile. This still broke with the same error (/bin/bash instead of /bin/sh). Found this solution for dealing with unset variables, and changed it to:\ndocker: $(call check_defined, VERSION, Please set a version number) @echo \u0026#34;Preparing to build version [${VERSION}] of cobusbernard/jekyll-blog container...\u0026#34; check_defined = \\ $(strip $(foreach 1,$1, \\ $(call __check_defined,$1,$(strip $(value 2))))) __check_defined = \\ $(if $(value $1),, \\ $(error Undefined $1$(if $2, ($2)))) Yay! Success!\n~/Projects/Sites/cobus.io git:(master) ✗ 17-02-05 9:11 make docker Makefile:10: *** Undefined VERSION (Please set a version number). Stop. ~/Projects/Sites/cobus.io git:(master) ✗ 17-02-05 9:12 VERSION=1.0.0 make docker Preparing to build version [1.0.0] of cobusbernard/jekyll-blog container... Building the docker image Now for the Dockerfile. I prefer building my own containers from the base OS ones as it helps me understand what is required to run a piece of software. I added the previous install_jekyll.sh commands (along with the cleanup suggested by Docker)\nFROM ubuntu:14.04.5 RUN apt-get update \\ \u0026amp;\u0026amp; apt-get install -y software-properties-common RUN add-apt-repository -y ppa:brightbox/ruby-ng RUN apt-get update RUN apt-get -y upgrade RUN apt-get install -y ruby2.1 ruby2.1-dev make gcc nodejs zlib1g-dev RUN gem install jekyll --no-rdoc --no-ri \\ \u0026amp;\u0026amp; gem install github-pages --no-rdoc --no-ri RUN rm -rf /var/lib/apt/lists/* I usually start with very many RUN commands as our internet here in South Africa still isn\u0026rsquo;t great (I have a ~20mbit LTE line) that is shared with everyone in the house. It is better to group the commands together to have less layers in the final image. There is a new feature in 1.13 of docker that allows squashing layers, I need to read up on that later.\nSurprise, this also broke. Looks like I need to upgrade my Ruby version from 2.1 to 2.2 according to this post. Once that was fixed, the container was built.\n~/Projects/Sites/cobus.io git:(master) ✗ 17-02-05 11:40 VERSION=1.0.0 make docker Preparing to build version [1.0.0] of cobusbernard/jekyll-blog container... Sending build context to Docker daemon 8.935 MB Step 1/9 : FROM ubuntu:14.04.5 ---\u0026gt; b969ab9f929b Step 2/9 : RUN apt-get update \u0026amp;\u0026amp; apt-get install -y software-properties-common ---\u0026gt; Using cache ---\u0026gt; 7823e8cc46fa Step 3/9 : RUN add-apt-repository -y ppa:brightbox/ruby-ng ---\u0026gt; Using cache ---\u0026gt; d5b1d17be061 Step 4/9 : RUN apt-get update ---\u0026gt; Using cache ---\u0026gt; 60a6cafc256b Step 5/9 : RUN apt-get -y upgrade ---\u0026gt; Using cache ---\u0026gt; 05f66007ae38 Step 6/9 : RUN apt-get install -y ruby2.2 ruby2.2-dev make gcc nodejs zlib1g-dev ---\u0026gt; Using cache ---\u0026gt; 30bad4f00fb3 Step 7/9 : RUN gem install jekyll --no-rdoc --no-ri \u0026amp;\u0026amp; gem install github-pages --no-rdoc --no-ri ---\u0026gt; Using cache ---\u0026gt; 9d9c6af564df Step 8/9 : RUN rm -rf /var/lib/apt/lists/* ---\u0026gt; Using cache ---\u0026gt; 3d7490d97763 Step 9/9 : ENTRYPOINT jekyll serve --watch /site ---\u0026gt; Using cache ---\u0026gt; 12c2ffb6c08d Successfully built 12c2ffb6c08d Once we have the container, we want to make it available to use again in future, so I created a repository for it under cobusbernard/jekyll-blog. To push to the repo, you will need to tag the container. Most repos will have a latest tag to use along with versioned ones. Adding the following to the Makefile target achieves that:\n@echo \u0026#34;Tagging latest and pushing to Docker hub...\u0026#34; docker tag jekyll-blog:latest cobusbernard/jekyll-blog:latest docker push cobusbernard/jekyll-blog:latest @echo \u0026#34;Tagging version ${VERSION} and pushing to Docker hub...\u0026#34; docker tag jekyll-blog:latest cobusbernard/jekyll-blog:${VERSION} docker push cobusbernard/jekyll-blog:${VERSION} Lastly, we need to add an ENTRYPOINT for the container. Have a read here for the difference between an ENTRYPOINT and a CMD - in my case, I don\u0026rsquo;t want to easily allow overwriting the command executed when the container starts as I like convention over configuration. When running the jekyll serve command, we need to ensure we are inside the correct directory. For that, we need to add a WORKDIR directive. The final Dockerfile looks like this:\nFROM ubuntu:14.04.5 RUN apt-get update \\ \u0026amp;\u0026amp; apt-get install -y software-properties-common RUN add-apt-repository -y ppa:brightbox/ruby-ng RUN apt-get update RUN apt-get -y upgrade RUN apt-get install -y ruby2.2 ruby2.2-dev make gcc nodejs zlib1g-dev RUN gem install jekyll --no-rdoc --no-ri \\ \u0026amp;\u0026amp; gem install github-pages --no-rdoc --no-ri RUN rm -rf /var/lib/apt/lists/* WORKDIR \u0026#34;/site\u0026#34; ENTRYPOINT [\u0026#34;jekyll\u0026#34;, \u0026#34;serve\u0026#34;, \u0026#34;--watch\u0026#34;, \u0026#34;/site\u0026#34;] It is safe to push this to a public Docker repository as there are no sensitive parts to it. When you start copying your code into a container, you might want to reconsider. Ideally if you are following the 12-factor app approach and you don\u0026rsquo;t have any configuration secrets in your container. This might not be the only sensitive information though, i.e. your code-base might be considered sensitive if it has algorithms written in Python in it. It would be trivial to copy out the code from the container if that was the case.\nRunning the container and the blog The last step is to flesh out the run target to use this new container to run the blog. The commands look something like this in normal bash:\ndocker stop jekyll-blog docker rm jekyll-blog docker run -tp 4000:4000 -v \u0026lt;current directory\u0026gt;:/site --name jekyll-blog cobusbernard/jekyll-blog:1.0.0 The first 2 will fail the first time you run them as you haven\u0026rsquo;t created a container with the name jekyll-blog before. On subsequent runs, they will succeed. So the first step is to indicate to make that the command might fail and to ignore that, this post explains how to do that in typical StackOverflow style: the marked answer is incorrect and the one below it with the most up-votes is the correct one. We also need to pass the current directory to make, usually you would use $(pwd). This post shows how to use $(shell pwd) for that purpose. The final Makefile looks like this:\nSHELL:=/bin/bash default: @echo \u0026#34;This file is used to work with the jekyll-blog Jekyll blog site.\u0026#34; @echo \u0026#34;The following commands are available:\u0026#34; @echo \u0026#34; - docker : builds the docker container to run the site locally.\u0026#34; @echo \u0026#34; - run : runs the docker container with the site\u0026#34; docker: $(call check_defined, VERSION, Please set a version number) @echo \u0026#34;Preparing to build version [${VERSION}] of cobusbernard/jekyll-blog container...\u0026#34; docker build -t jekyll-blog . @echo \u0026#34;Tagging latest and pushing to Docker hub...\u0026#34; docker tag jekyll-blog:latest cobusbernard/jekyll-blog:latest docker push cobusbernard/jekyll-blog:latest @echo \u0026#34;Tagging version ${VERSION} and pushing to Docker hub...\u0026#34; docker tag jekyll-blog:latest cobusbernard/jekyll-blog:${VERSION} docker push cobusbernard/jekyll-blog:${VERSION} run: @echo \u0026#34;Running the docker container cobusbernard/jekyll-blog to start Jekyll...\u0026#34; -@docker stop jekyll-blog -@docker rm jekyll-blog docker run -tp 4000:4000 -v $(shell pwd):/site --name jekyll-blog cobusbernard/jekyll-blog:1.0.0 # Check that given variables are set and all have non-empty values, # die with an error otherwise. # # Params: # 1. Variable name(s) to test. # 2. (optional) Error message to print. check_defined = \\ $(strip $(foreach 1,$1, \\ $(call __check_defined,$1,$(strip $(value 2))))) __check_defined = \\ $(if $(value $1),, \\ $(error Undefined $1$(if $2, ($2)))) To run the site, I just execute make run. And viola:\n~/Projects/Sites/cobus.io git:(master) ✗ 17-02-05 12:19 make run Running the docker container cobusbernard/jekyll-blog to start Jekyll... jekyll-blog jekyll-blog Configuration file: /site/_config.yml Configuration file: /site/_config.yml Source: /site Destination: /site/_site Incremental build: disabled. Enable with --incremental Generating... done in 3.487 seconds. Auto-regeneration: enabled for \u0026#39;/site\u0026#39; Configuration file: /site/_config.yml Server address: http://0.0.0.0:4000/ Server running... press ctrl-c to stop. ","date":"February 5, 2017","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2017-02-05/yak-shaving-makefiles/","summary":"The Problem I woke up this morning wanting to start on a long post about Terraform and how I ended up with the structure shown in my repo. I recently reinstalled my Macbook using Netatalk for a timemachine , and as I fired up my vagrant image, it started to pull down the Ubuntu box first and then started configuring it. This is very useful, but it is slow. And it broke.","tags":["docker","osx","make","scripts"],"title":"Yak Shaving - Makefiles"},{"categories":["Docker"],"contents":"I am currently unable to upgrade to the latest version of Slack due to this bug. That combined with my user profile confusion (originally I set up using a US account, then changed to an South African one) that causes requests to update to versions of iPhoto which aren\u0026rsquo;t available in my region made me decide that it is time for a reformat.\nTo create a full backup to my raid\u0026rsquo;ed server, I need to set up netatalk. I started reading this post but decided that I didn\u0026rsquo;t want to pollute my server with all of those installs. Seeing that I already have Mesos and Marathon running, I went the docker route. I found this repo and proceeded to create the following Marathon config:\n{ \u0026#34;id\u0026#34;: \u0026#34;netatalk\u0026#34;, \u0026#34;instances\u0026#34;: 1, \u0026#34;cpus\u0026#34;: 0.2, \u0026#34;mem\u0026#34;: 256.0, \u0026#34;constraints\u0026#34;: [[\u0026#34;hostname\u0026#34;, \u0026#34;CLUSTER\u0026#34;, \u0026#34;myhost.mydoamin.com\u0026#34;]], \u0026#34;container\u0026#34;: { \u0026#34;docker\u0026#34;: { \u0026#34;image\u0026#34;: \u0026#34;cptactionhank/netatalk:latest\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;HOST\u0026#34;, \u0026#34;portMappings\u0026#34;: [ { \u0026#34;containerPort\u0026#34;: 548, \u0026#34;protocol\u0026#34;: \u0026#34;tcp\u0026#34; } ] }, \u0026#34;type\u0026#34;: \u0026#34;DOCKER\u0026#34;, \u0026#34;volumes\u0026#34;: [ { \u0026#34;hostPath\u0026#34;: \u0026#34;/data/media\u0026#34;, \u0026#34;containerPath\u0026#34;: \u0026#34;/media/share\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;RW\u0026#34; }, { \u0026#34;hostPath\u0026#34;: \u0026#34;/data/timemachine\u0026#34;, \u0026#34;containerPath\u0026#34;: \u0026#34;/media/timemachine\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;RW\u0026#34; } ] }, \u0026#34;env\u0026#34;: { \u0026#34;AVAHI\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;AFP_USER\u0026#34;: \u0026#34;timemachine\u0026#34;, \u0026#34;AFP_PASSWORD\u0026#34;: \u0026#34;super_secret\u0026#34;, \u0026#34;AFP_UID\u0026#34;: \u0026#34;1012\u0026#34;, \u0026#34;AFP_GID\u0026#34;: \u0026#34;1007\u0026#34; }, \u0026#34;healthChecks\u0026#34;: [ { \u0026#34;gracePeriodSeconds\u0026#34;: 60, \u0026#34;intervalSeconds\u0026#34;: 30, \u0026#34;maxConsecutiveFailures\u0026#34;: 0, \u0026#34;port\u0026#34;: 548, \u0026#34;protocol\u0026#34;: \u0026#34;TCP\u0026#34; } ] } The important parts are Limiting where the container may run, in my case, I only want it on my server with the raid array:\n\u0026#34;constraints\u0026#34;: [[\u0026#34;hostname\u0026#34;, \u0026#34;CLUSTER\u0026#34;, \u0026#34;myhost.mydoamin.com\u0026#34;]] Netatalk runs on port tcp/548:\n\u0026#34;containerPort\u0026#34;: 548 If you want to use service discovery, you need to have the container run on the host network stack:\n\u0026#34;network\u0026#34;: \u0026#34;HOST\u0026#34; You can also use netatalk to share media easily, so I mounted my media folder to share as well:\n\u0026#34;hostPath\u0026#34;: \u0026#34;/data/media\u0026#34;, \u0026#34;containerPath\u0026#34;: \u0026#34;/media/share\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;RW\u0026#34; The folder that timemachine will use to back up:\n\u0026#34;hostPath\u0026#34;: \u0026#34;/data/timemachine\u0026#34;, \u0026#34;containerPath\u0026#34;: \u0026#34;/media/timemachine\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;RW\u0026#34; The environment variables for the container:\n\u0026#34;env\u0026#34;: { \u0026#34;AVAHI\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;AFP_USER\u0026#34;: \u0026#34;timemachine\u0026#34;, \u0026#34;AFP_PASSWORD\u0026#34;: \u0026#34;super_secret\u0026#34;, \u0026#34;AFP_UID\u0026#34;: \u0026#34;1012\u0026#34;, \u0026#34;AFP_GID\u0026#34;: \u0026#34;1007\u0026#34; }, Service discovery is set with AVAHI=1 and the other values are for credentials for timemachine. You will need to create a user on the server and get the userid and groupid for it, i.e.:\nsudo adduser --home /mnt/data/timemachine timemachine sudo chown -R timemachine:timemachine /mnt/data/timemachine To get the userid and groupid to use, run the following:\nsudo -H -u timemachine bash -c \u0026#39;echo \u0026#34;I am $(id -un), with uid $(id -u) and gid $(id -g)\u0026#34;\u0026#39; And that is all folks, it takes 5 minutes to set up a timemachine server using Docker, Mesos and Marathon. After the container has started up, head over to timemachine and you should see it listed when you click on the Select disk... button. Now I just need to add the user creation to my Chef recipes so I don\u0026rsquo;t forget about it when I recreate the node\u0026hellip;\n","date":"January 15, 2017","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2017-01-15/timemachine-backups/","summary":"I am currently unable to upgrade to the latest version of Slack due to this bug. That combined with my user profile confusion (originally I set up using a US account, then changed to an South African one) that causes requests to update to versions of iPhoto which aren\u0026rsquo;t available in my region made me decide that it is time for a reformat.\nTo create a full backup to my raid\u0026rsquo;ed server, I need to set up netatalk.","tags":["docker","mesos","marathon","apply","osx","home"],"title":"Setting up a Timemachine server using Mesos/Marathon"},{"categories":["AWS"],"contents":"I\u0026rsquo;ve been working with multiple AWS accounts for the last few months between various organisations. Logging into each one when I need to make a change quickly became tedious and slow. Each environment (dev, test, staging, production) has their own AWS account. The need to log in stems from taming the infrastructure with Terraform for systems that have been set up by hand and dealing with the discrepancies between them, so I tend to jump between dev and staging very often. Being in this is a very normal state of affairs as most people starting out with AWS haven\u0026rsquo;t worked with any infrastructure creation automation. I will create another post detailing how to start this taming process.\nThe second benefit of having multiple environments is that you can consolidate billing across all of them, benefit from AWS volume discounts and see the breakdown between environments between different accounts. You also have better isolation as permissions are delegated across accounts and you can easily create/lock an account, or in the event of a breach, isolate an entire environment by revoking permission.\nWe are going to set up 3 new AWS accounts, delegate billing to the master one and have cross account administrator access for a user.\nFirst step is setting up 3 AWS accounts via the sig-nup page. As each account requires a unique email address, the use of a + in an email address is very useful (allowed via RFC 2822). For amazon@yourdomain.com, I suggest something like:\namazon+master@yourdomain.com amazon+development@yourdomain.com amazon+production@yourdomain.com This will allow you to have all the emails in a single inbox. For security, I would advise to have different emails for each account, but it complicates things for this demo.\nWhen you click on register, it will ask you for your name, use something like AWS Master for the master account as it will make identifying the accounts in the console easier, see the screenshot below. For the rest of the registration, use your proper details (name / surname). Registration will require you to provide a credit card (they will charge with $1.00 to confirm it is valid) and a valid phone number. During registration, they will call you on this number and you have to type in the on-screen pin to verify the phone number. For the support plan, choose the basic one for now - it is free. Your account is now ready, but you are not logged in, do so now. Spend some time reading through the free tier documentation, you get a decent amount for the first 12 months to help you start out. Log out and create accounts for development and production as well.\nThis is where the account name goes:\nAnd this is how it will display on the top-right side of the screen after logging in:\nFirst thing we want to do is set up the consolidated billing. This is done on the master account only, click on your account name on the top-right of the screen, then on Billing \u0026amp; Cost Management:\nOn the following screen, click on Consolidated Billing on the left-hand side:\nEnable delegated billing by clicking on the Sign up for Consolidated Billing. The button text will change to indicate it is being set up and, once complete, a green notification will dropdown in the top-middle of the screen. The button will revert back to Sign up for Consolidated Billing, you will need to refresh the page to configure it. Click the Send a Request button, enter the development account\u0026rsquo;s email address, i.e. amazon+development@example.com, and click on send. Do the same for the production account. You now need to log out of the master account, go to your email and click on the link there to enable the billing consolidation. The link will take you to the console sign-in page, use the credentials for the development / production account, depending on which link you clicked on first. After logging in, you will need to click on the Accept request button. It wil grey out and indicate it is processing. Once complete, you will see the accepted state:\nWhen you log back into your master account, you will see the following in the consolidated billing section:\nWrite down the account Id values for development and production, we will need them for the role delegation. Now it is time for setting up your daily account. You should never log in with your root account credentials unless something has happened to this account we are about to set up. By default, user accounts (even ones with administrator roles), cannot access the billing section as a security measure. With the root credentials, a malicious person can change your password and the email used to log in with, locking you out of your account completely. They can then abuse it until you realise this and contact AWS support to regain access to your account - this takes time and will further increase your bill. You should also set up multi-factor authentication on you account.\nThe cross account permission delegation has two components to it, the first is where the root account grants access to another root account to assume a role inside itself. I.e. the development root account has a role administrator that has the built-in AdministratorAccess IAM policy attached to it. This role is then set up to allow the master account to assume it. On the master account side, you will create a new group with permissions to assume this role and add a user to this group. The layered security here has 2 benefits:\nIf your master account is compromised, you can revoke access to the environments from their accounts by removing the role permission. If you need to add or remove a user\u0026rsquo;s access to your systems, it is done in a single place (your master account) Log back in into your master account. Click on Services on the top-left side, then \u0026lsquo;All AWS Services\u0026rsquo;, then IAM. It is also useful to click on the edit button and add the services you access regularly - they will also be visible at the top of the console. Create a new policy by clicking on Policies on the left, then the Create Policy button. Choose Create your own policy. Use development_administrator for the name, any description you want, i.e. Allows assuming the development administration role on the development account. and the policy itself as (remember to replace development_account_id with your account id):\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::development_account_id:role/administrator\u0026#34; } } Do the same for production. Go to Groups on the left and create a new group called development_administrator. On then next screen, click on Policy Type and select Customer Managed Policies. You will see the development_administrator and production_administrator ones there, click on the check box to the left of the development one. Click on next. A final confirmation screen will appear, showing:\nAfter clicking on Create Group, it will show up in the list:\nAgain, do the same for the production account. Create a new user for yourself, I will call mine cobus by clicking on Users on the left-hand side and then on Create New User. The tick box for Generate access key for each user will be ticket by default. Click on next and then on the arrow to show your credentials. Copy the values, or download the file - I prefer to copy and paste to a safe location immediately instead of downloading the file as you might leave it in your download folder and forget about it. For now, place them in the file ~/.aws/credentials:\n[demo-master-cobus] aws_access_key_id=AKIAJA6ZCX6DGYD2IJJQ aws_secret_access_key=Q/5Rb4UV1B7sMayNJDBL3Au1wZBTiypBQO+dszZi (The credentials above have already been revoked, I decided to include actual ones to avoid formatting confusion). This allows you to interact with the AWS CLI via aws --profile demo-master-cobus. It is safer to not set a [default] profile as the CLI will use that when you do not specify --profile. As I work with many accounts, this is a risk that I prefer to avoid. Click on close twice to create the user. Select the user from the list and go to the Security Credentials tab.\nHere you can see the user and that it hasn\u0026rsquo;t been assigned a password yet. Click on Manage Password and generate one for the user. You can decide to provide one, or have it generated. Entirely up to your preference. Once again, copy \u0026amp; paste the credentials to a safe place - I use 1password religiously for this. You will also want to have administrator rights on the master account, so create a new group called master_administrator and attach the built-in AdministratorAccess policy to it. Now that we have a user, it is time to give it some permissions via groups. Click on the Groups tab, tick both of the groups you created earlier and click on Add to groups.\nIt is almost time to log in with the user, but first, let\u0026rsquo;s make that easier. Go back to the top level of IAM by clicking on Dashboard on the left. You will see the following:\nYou will see a number in the link, this is your account ID for the master account, write it down for later. Click on the customize link at the top/mid-right of the screen. This will allow you to choose a friendly name for your login, i.e. https://friendly-name.signin.aws.amazon.com/console. Choose something sensible for your account. You will be using this link to log in in future.\nWe still haven\u0026rsquo;t given our master account permission to assume roles in the other two accounts, let\u0026rsquo;s do this now. Log out of this account and back into the development one. Go to the IAM section, and then to Roles and finally click on Create New Role. Use administrator for this role - this needs to match what you specified in the policy earlier for your development_administrator policy. On the next screen, select Role for Cross-Account Access and then Provide access between AWS accounts you own. Enter the master account ID on the next screen. You can tick the Require MFA option - this will force the user to have MFA enable on their user in the master account before being allowed to switch to this role. In the next step, choose the built-in AdministratorAccess policy. On the last step, you can review the setup, it should look like this:\nCopy the link provided, log out and do the same for the production account. It is finally time to use the new login. Enter the friendly url you created earlier after logging out of the production account, i.e. https://friendly-name.signin.aws.amazon.com/console:\nYou will see your IAM account name and friendly login name at the top right as cobus @ friendly-name. In a new tab, paste the link you copied when creating the cross account role: https://signin.aws.amazon.com/switchrole?account=development_account_id\u0026amp;roleName=administrator. It will bring up 3 boxes to fill in, the Account and Role will already be populated with values. Choose a short name for the development account, i.e. Development and a colour - I use green for dev, yellow for staging / uat and red for production environments. You will see why when we go back to the first tab. Click on Switch role, wait for the screen to load, then close that tab. Open another new one and do the same for production. Close the tab when done.\nGo back to your first tab and refresh the page. There will be a dialog indicating that says:\nYou will see a red oval top-right with Production in it, when you click on it, you will see:\nYou are now able to switch between development and production without having to log in! If you want to go back to the master account, simply click on Back to cobus in that menu. What is happening in the background is the console uses your master account user to generate temporary credentials on the development / production account via the STS service and using them to access the other environments.\nFinally, you want to configure profiles to do the role switching for you when using the CLI. To do this, you can configure them in ~/.aws/config\n[profile demo-master-cobus] region = us-west-2 [profile demo-development-cobus] role_arn = arn:aws:iam::\u0026lt;development_account_id\u0026gt;:role/administrator source_profile = demo-master-cobus [profile demo-production-cobus] role_arn = arn:aws:iam::\u0026lt;production_account_id\u0026gt;:role/administrator source_profile = demo-master-cobus This will allow you to add --profile \u0026lt;target\u0026gt; to your CLI commands, i.e. aws --profile demo-master-cobus s3 cp s://my-bucket/myfile.txt ./ to copy from a bucket created in the master account. Or if you want to copy from a bucket in the development account: aws --profile demo-development-cobus s3://my-dev-bucket/myfile.txt ./. You are now able to update each individual AWS root account using a single credential set. When managing many users, this becomes very powerful as you only have a single location to look at to assess what a user\u0026rsquo;s rights are across all your environments.\n","date":"September 3, 2016","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2016-09-03/aws-multi-account/","summary":"I\u0026rsquo;ve been working with multiple AWS accounts for the last few months between various organisations. Logging into each one when I need to make a change quickly became tedious and slow. Each environment (dev, test, staging, production) has their own AWS account. The need to log in stems from taming the infrastructure with Terraform for systems that have been set up by hand and dealing with the discrepancies between them, so I tend to jump between dev and staging very often.","tags":["aws","security","terraform","infrastructure-as-code"],"title":"Securing AWS environments using role switching"},{"categories":["Linux"],"contents":"I\u0026rsquo;ve been working on a pair of Centos servers using GluserFS for a volume that is shared by various other servers. Each time the server reboots, I had to log in and manually start the service. Turns out this is due to the networking no yet being started when the glusterd service starts. I found this post with the solution:\nExecute systemctl enable NetworkManager-wait-online Add the following to /lib/systemd/system/crond.service under [Unit]: Requires=network.target After=syslog.target auditd.service systemd-user-sessions.service time-sync.target network.target mysqld.service This will allow glusterd to be started after the network has come up.\n","date":"June 12, 2016","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2016-06-12/centos-gluster-startup/","summary":"I\u0026rsquo;ve been working on a pair of Centos servers using GluserFS for a volume that is shared by various other servers. Each time the server reboots, I had to log in and manually start the service. Turns out this is due to the networking no yet being started when the glusterd service starts. I found this post with the solution:\nExecute systemctl enable NetworkManager-wait-online Add the following to /lib/systemd/system/crond.service under [Unit]: Requires=network.","tags":["centos","glusterfs","linux"],"title":"Getting Glusterd to start at boot on Centos"},{"categories":["DevOps"],"contents":"I just upgraded my chef-clients via the omnibus_updater cookbook when things started breaking:\n192.168.5.5 [2016-02-18T21:41:13+02:00] WARN: Current apt_package[apt-transport-https]: /var/chef/cache/cookbooks/datadog/recipes/repository.rb:24:in `from_file\u0026#39; 192.168.5.5 192.168.5.5 ================================================================================ 192.168.5.5 Recipe Compile Error in /var/chef/cache/cookbooks/chef-wrapper-omnibus-updater/recipes/default.rb 192.168.5.5 ================================================================================ 192.168.5.5 192.168.5.5 NameError 192.168.5.5 --------- 192.168.5.5 uninitialized constant Chef::REST 192.168.5.5 192.168.5.5 Cookbook Trace: 192.168.5.5 --------------- 192.168.5.5 /var/chef/cache/cookbooks/omnibus_updater/libraries/omnitrucker.rb:84:in `url\u0026#39; 192.168.5.5 /var/chef/cache/cookbooks/omnibus_updater/recipes/downloader.rb:27:in `from_file\u0026#39; 192.168.5.5 /var/chef/cache/cookbooks/omnibus_updater/recipes/default.rb:25:in `from_file\u0026#39; 192.168.5.5 /var/chef/cache/cookbooks/chef-wrapper-omnibus-updater/recipes/default.rb:27:in `from_file\u0026#39; 192.168.5.5 192.168.5.5 Relevant File Content: 192.168.5.5 ---------------------- 192.168.5.5 /var/chef/cache/cookbooks/omnibus_updater/libraries/omnitrucker.rb: 192.168.5.5 192.168.5.5 77: if(url_or_node.is_a?(Chef::Node)) 192.168.5.5 78: url = build_url(url_or_node) 192.168.5.5 79: node = url_or_node 192.168.5.5 80: else 192.168.5.5 81: url = url_or_node 192.168.5.5 82: raise \u0026#34;Node instance is required for Omnitruck.url!\u0026#34; unless node 192.168.5.5 83: end 192.168.5.5 84\u0026gt;\u0026gt; request = Chef::REST::RESTRequest.new(:head, URI.parse(url), nil) 192.168.5.5 85: result = request.call 192.168.5.5 86: if(result.kind_of?(Net::HTTPRedirection)) 192.168.5.5 87: result[\u0026#39;location\u0026#39;] 192.168.5.5 88: end 192.168.5.5 89: end 192.168.5.5 90: 192.168.5.5 91: end 192.168.5.5 92: end 192.168.5.5 93: Googling for uninitialized constant Chef::REST only yielded some results from March 2013, so I decided to upgrade the omnibus_updater cookbook from 1.0.4. to 1.0.6 by editing metadata.rb. I bumped the dependency version as well as my wrapper cookbook version, but when I ran berks install, I got the following:\nResolving cookbook dependencies... Fetching \u0026#39;chef-wrapper-omnibus-updater\u0026#39; from source at . Fetching cookbook index from https://supermarket.chef.io... Unable to satisfy constraints on package omnibus_updater due to solution constraint (omnibus_updater = 1.0.4). Solution constraints that may result in a constraint on omnibus_updater: [(chef-wrapper-omnibus-updater = 0.1.2) -\u0026gt; (omnibus_updater ~\u0026gt; 1.0.6)], [(omnibus_updater = 1.0.4)] Demand that cannot be met: (omnibus_updater = 1.0.4) Artifacts for which there are conflicting dependencies: omnibus_updater = 1.0.4 -\u0026gt; []Unable to find a solution for demands: chef-wrapper-omnibus-updater (0.1.2), omnibus_updater (1.0.4) That at least had some Google results, ends up that I need to upgrade via berks update omnibus_updater instead of just incrementing the version number. After the upgrade, the other issue was also resolved.\n","date":"February 18, 2016","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2016-02-18/upgrading-berks-dependency-issue/","summary":"I just upgraded my chef-clients via the omnibus_updater cookbook when things started breaking:\n192.168.5.5 [2016-02-18T21:41:13+02:00] WARN: Current apt_package[apt-transport-https]: /var/chef/cache/cookbooks/datadog/recipes/repository.rb:24:in `from_file\u0026#39; 192.168.5.5 192.168.5.5 ================================================================================ 192.168.5.5 Recipe Compile Error in /var/chef/cache/cookbooks/chef-wrapper-omnibus-updater/recipes/default.rb 192.168.5.5 ================================================================================ 192.168.5.5 192.168.5.5 NameError 192.168.5.5 --------- 192.168.5.5 uninitialized constant Chef::REST 192.168.5.5 192.168.5.5 Cookbook Trace: 192.168.5.5 --------------- 192.168.5.5 /var/chef/cache/cookbooks/omnibus_updater/libraries/omnitrucker.rb:84:in `url\u0026#39; 192.168.5.5 /var/chef/cache/cookbooks/omnibus_updater/recipes/downloader.rb:27:in `from_file\u0026#39; 192.168.5.5 /var/chef/cache/cookbooks/omnibus_updater/recipes/default.rb:25:in `from_file\u0026#39; 192.168.5.5 /var/chef/cache/cookbooks/chef-wrapper-omnibus-updater/recipes/default.rb:27:in `from_file\u0026#39; 192.168.5.5 192.168.5.5 Relevant File Content: 192.168.5.5 ---------------------- 192.168.5.5 /var/chef/cache/cookbooks/omnibus_updater/libraries/omnitrucker.rb: 192.168.5.5 192.168.5.5 77: if(url_or_node.","tags":["chef"],"title":"Upgrading a Chef cookbook with Berkshelf"},{"categories":["DevOps"],"contents":"Most people would have experienced the issue of setting values in web.config for a project on different environments, i.e. the connection string for the database. My first attempt at resolving this was to simply create multiple configurations and build the appropriate one per environment. This has multiple issues: you are including sensitive information in your build artifact, creating different builds for the same version (to allow different values) and tightly coupling your build process to your environment values. This will require a rebuild if your connection string changes, which could be problematic if you are unable to build a specific version easily. But it won\u0026rsquo;t be the exact same version as you would need to commit the new config value, re-tag and finally rebuild the solution. Sounds like too much effort for a simple change.\nCreating a project to test this First, let\u0026rsquo;s create a new solution to work with. Create project -\u0026gt; Web -\u0026gt; ASP.Net Web Application\nSave the project and close it. Your folder should look like this:\nYou often have additional files to store in your version control system, so I prefer to keep all my source in a sub-folder called src. Move the files into the directory and you should have a folder structure like this:\nAdding the parameters.xml file There is feature that isn\u0026rsquo;t very well known: you can set parameters in the web.config at deployment time via a SetParameters.xml file if you are creating your build artifact via MSBuild. If you want to add additional parameters into it, add parameters.xml to the project you are publishing.\nNow add the following content to it:\n\u0026lt;parameters\u0026gt; \u0026lt;parameter name=\u0026#34;IIS Web Application Name\u0026#34; defaultValue=\u0026#34;Default Web Site/WebApi.Template\u0026#34;/\u0026gt; \u0026lt;parameter name=\u0026#34;NHibernate Connection string\u0026#34; defaultValue=\u0026#34;\u0026#34; \u0026gt; \u0026lt;parameterEntry kind=\u0026#34;XmlFile\u0026#34; scope=\u0026#34;Website\\\\Web\\.config$\u0026#34; match=\u0026#34;/configuration/connectionStrings/add[@name=\u0026#39;NHibernateConnection\u0026#39;]/@connectionString\u0026#34; /\u0026gt; \u0026lt;/parameter\u0026gt; \u0026lt;parameter name=\u0026#34;LOGENTRIES_TOKEN\u0026#34; defaultValue=\u0026#34;\u0026#34;\u0026gt; \u0026lt;parameterEntry kind=\u0026#34;XmlFile\u0026#34; scope=\u0026#34;Website\\\\Web\\.config$\u0026#34; match=\u0026#34;/configuration/appSettings/add[@key=\u0026#39;LOGENTRIES_TOKEN\u0026#39;]/@value\u0026#34; /\u0026gt; \u0026lt;/parameter\u0026gt; \u0026lt;/parameters\u0026gt; The important parts to note here are:\nname=\u0026quot;\u0026lt;parameter name\u0026gt;\u0026quot; - Allows giving the parameter a human readable name, i.e. NHibnernation Connection string defaultValue=\u0026quot;\u0026quot; - you can specify a default value to use if there is none in the SetParameters.xml file for it, i.e. INFO as the log level if not specified. Leaving it empty will enforce adding a value to the SetParameters.xml file. scope=\u0026quot;Website\\\\Web\\.config$\u0026quot; - Specifies the file to search using the match string. The default packaging does not put the files in this folder structure, more on that further down. match=\u0026quot;/configuration/connectionStrings/add[@name='NHibernateConnection']/@connectionString\u0026quot; /\u0026gt; - The location of the value to set using the value we provide. Once again, more on this further down. Creating a publishing profile This will allow you to add additional parameters to SetParameters.xml. To test what this does, we can use the publish feather in Visual Studio to publish to a local file. Create a publishing profile by right-clicking on the WebApi.Template project and selecting Publish and click on Custom. Use the following values which are relative to your project file. This is where the folder structure starts to help, we will be creating a dist folder next to the src one to which we will publish the project. Once we add the solution to source control, we can exclude the dist folder to avoid committing compiled code.\nAfter clicking finish, you will should see the following in the build output at the bottom of Visual Studio:\n1\u0026gt;------ Build started: Project: WebApi.Template, Configuration: Release Any CPU ------ 1\u0026gt; WebApi.Template -\u0026gt; V:\\Personal\\WebApi.Template\\src\\WebApi.Template\\bin\\WebApi.Template.dll 2\u0026gt;------ Publish started: Project: WebApi.Template, Configuration: Release Any CPU ------ 2\u0026gt;Transformed Web.config using V:\\Personal\\WebApi.Template\\src\\WebApi.Template\\Web.Release.config into obj\\Release\\TransformWebConfig\\transformed\\Web.config. 2\u0026gt;Auto ConnectionString Transformed Areas\\HelpPage\\Views\\Web.config into obj\\Release\\CSAutoParameterize\\transformed\\Areas\\HelpPage\\Views\\Web.config. 2\u0026gt;Auto ConnectionString Transformed Views\\Web.config into obj\\Release\\CSAutoParameterize\\transformed\\Views\\Web.config. 2\u0026gt;Auto ConnectionString Transformed obj\\Release\\TransformWebConfig\\transformed\\Web.config into obj\\Release\\CSAutoParameterize\\transformed\\Web.config. 2\u0026gt;Copying all files to temporary location below for package/publish: 2\u0026gt;obj\\Release\\Package\\PackageTmp. 2\u0026gt;Packaging into V:\\Personal\\WebApi.Template\\src\\dist\\WebApi.Template.zip. 2\u0026gt;Adding sitemanifest (sitemanifest). 2\u0026gt;Adding IIS Application (V:\\Personal\\WebApi.Template\\src\\WebApi.Template\\obj\\Release\\Package\\PackageTmp) 2\u0026gt;Creating application (V:\\Personal\\WebApi.Template\\src\\WebApi.Template\\obj\\Release\\Package\\PackageTmp) 2\u0026gt;Adding virtual path (V:\\Personal\\WebApi.Template\\src\\WebApi.Template\\obj\\Release\\Package\\PackageTmp) 2\u0026gt;Adding directory (V:\\Personal\\WebApi.Template\\src\\WebApi.Template\\obj\\Release\\Package\\PackageTmp). ... 2\u0026gt;Adding file (V:\\Personal\\WebApi.Template\\src\\WebApi.Template\\obj\\Release\\Package\\PackageTmp\\Web.config). 2\u0026gt;Adding ACL\u0026#39;s for path (V:\\Personal\\WebApi.Template\\src\\WebApi.Template\\obj\\Release\\Package\\PackageTmp) 2\u0026gt;Adding ACL\u0026#39;s for path (V:\\Personal\\WebApi.Template\\src\\WebApi.Template\\obj\\Release\\Package\\PackageTmp) 2\u0026gt;Adding declared parameter \u0026#39;IIS Web Application Name\u0026#39;. 2\u0026gt;Adding declared parameter \u0026#39;NHibernate Connection string\u0026#39;. 2\u0026gt;Adding declared parameter \u0026#39;LOGENTRIES_TOKEN\u0026#39;. 2\u0026gt;Package \u0026#34;WebApi.Template.zip\u0026#34; is successfully created as single file at the following location: 2\u0026gt;file:///V:/Personal/WebApi.Template/src/dist 2\u0026gt;To get the instructions on how to deploy the web package please visit the following link: 2\u0026gt;http://go.microsoft.com/fwlink/?LinkId=124618 2\u0026gt;Sample script for deploying this package is generated at the following location: 2\u0026gt;V:\\Personal\\WebApi.Template\\src\\dist\\WebApi.Template.deploy.cmd 2\u0026gt;For this sample script, you can change the deploy parameters by changing the following file: 2\u0026gt;V:\\Personal\\WebApi.Template\\src\\dist\\WebApi.Template.SetParameters.xml ========== Build: 1 succeeded, 0 failed, 0 up-to-date, 0 skipped ========== ========== Publish: 1 succeeded, 0 failed, 0 skipped ========== You will see the following lines in the output pertaining to our new parameters file:\n2\u0026gt;Adding declared parameter \u0026#39;IIS Web Application Name\u0026#39;. 2\u0026gt;Adding declared parameter \u0026#39;NHibernate Connection string\u0026#39;. 2\u0026gt;Adding declared parameter \u0026#39;LOGENTRIES_TOKEN\u0026#39;. When you open up the zip, you will see the following:\nInside the SetParameters.xml you will see the following values:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;parameters\u0026gt; \u0026lt;setParameter name=\u0026#34;IIS Web Application Name\u0026#34; value=\u0026#34;Default Web Site/WebApi.Template\u0026#34; /\u0026gt; \u0026lt;setParameter name=\u0026#34;NHibernate Connection string\u0026#34; value=\u0026#34;\u0026#34; /\u0026gt; \u0026lt;setParameter name=\u0026#34;LOGENTRIES_TOKEN\u0026#34; value=\u0026#34;\u0026#34; /\u0026gt; \u0026lt;/parameters\u0026gt; Changing the publish folder structure By default, the location inside the zip file is fairly insane, here is what it would be for the project created above: WebApi.Template.zip\\Content\\V_C\\Personal\\WebApi.Template\\src\\WebApi.Template\\obj\\Release\\Package\\PackageTmp for the project that is stored in V:\\Personal\\WebApi.Template\\src. To address this, add the following to the end of your project file (I\u0026rsquo;ve included the default commented out before/after build targets help find the position):\n\u0026lt;!-- To modify your build process, add your task inside one of the targets below and uncomment it. Other similar extension points exist, see Microsoft.Common.targets. \u0026lt;Target Name=\u0026#34;BeforeBuild\u0026#34;\u0026gt; \u0026lt;/Target\u0026gt; \u0026lt;Target Name=\u0026#34;AfterBuild\u0026#34;\u0026gt; \u0026lt;/Target\u0026gt; --\u0026gt; \u0026lt;PropertyGroup\u0026gt; \u0026lt;PackagePath Condition=\u0026#34; \u0026#39;$(PackagePath)\u0026#39;==\u0026#39;\u0026#39; \u0026#34;\u0026gt;Website\u0026lt;/PackagePath\u0026gt; \u0026lt;EnableAddReplaceToUpdatePackagePath Condition=\u0026#34; \u0026#39;$(EnableAddReplaceToUpdatePackagePath)\u0026#39;==\u0026#39;\u0026#39; \u0026#34;\u0026gt;true\u0026lt;/EnableAddReplaceToUpdatePackagePath\u0026gt; \u0026lt;PackageDependsOn\u0026gt; $(PackageDependsOn); AddReplaceRuleForAppPath; \u0026lt;/PackageDependsOn\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;Target Name=\u0026#34;AddReplaceRuleForAppPath\u0026#34; Condition=\u0026#34; \u0026#39;$(EnableAddReplaceToUpdatePackagePath)\u0026#39;==\u0026#39;true\u0026#39; \u0026#34;\u0026gt; \u0026lt;PropertyGroup\u0026gt; \u0026lt;_PkgPathFull\u0026gt;$([System.IO.Path]::GetFullPath($(WPPAllFilesInSingleFolder)))\u0026lt;/_PkgPathFull\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;!-- escape the text into a regex --\u0026gt; \u0026lt;EscapeTextForRegularExpressions Text=\u0026#34;$(_PkgPathFull)\u0026#34;\u0026gt; \u0026lt;Output TaskParameter=\u0026#34;Result\u0026#34; PropertyName=\u0026#34;_PkgPathRegex\u0026#34; /\u0026gt; \u0026lt;/EscapeTextForRegularExpressions\u0026gt; \u0026lt;!-- add the replace rule to update the path --\u0026gt; \u0026lt;ItemGroup\u0026gt; \u0026lt;MsDeployReplaceRules Include=\u0026#34;replaceFullPath\u0026#34;\u0026gt; \u0026lt;Match\u0026gt;$(_PkgPathRegex)\u0026lt;/Match\u0026gt; \u0026lt;Replace\u0026gt;$(PackagePath)\u0026lt;/Replace\u0026gt; \u0026lt;/MsDeployReplaceRules\u0026gt; \u0026lt;/ItemGroup\u0026gt; \u0026lt;/Target\u0026gt; Now when we publish, the directory structure inside the archive will be: WebApi.Template.zip\\Content\\Website. This ties in the value specified in parameters.xml for the scope element.\nXML Matching in Web.config Earlier in this post, there was an element in the parameters.xml file called match with the following 2 values:\nmatch=\u0026#34;/configuration/connectionStrings/add[@name=\u0026#39;NHibernateConnection\u0026#39;]/@connectionString\u0026#34; /\u0026gt; match=\u0026#34;/configuration/appSettings/add[@key=\u0026#39;LOGENTRIES_TOKEN\u0026#39;]/@value\u0026#34; /\u0026gt; This is used to lookup values in the web.config with a structure like this:\n\u0026lt;configuration\u0026gt; \u0026lt;connectionStrings\u0026gt; \u0026lt;add name=\u0026#34;NHibernateConnection\u0026#34; connectionString=\u0026#34;Data Source=localhost;Initial Catalog=WebApiTemplate;Integrated Security=True\u0026#34; providerName=\u0026#34;System.Data.SqlClient\u0026#34; /\u0026gt; \u0026lt;/connectionStrings\u0026gt; \u0026lt;appSettings\u0026gt; \u0026lt;add key=\u0026#34;LOGENTRIES_TOKEN\u0026#34; value=\u0026#34;123456\u0026#34; /\u0026gt; \u0026lt;add key=\u0026#34;SomeRandomValue\u0026#34; value=\u0026#34;123456\u0026#34; /\u0026gt; \u0026lt;/appSettings\u0026gt; \u0026lt;customValues\u0026gt; \u0026lt;value name=\u0026#34;MyField\u0026#34;\u0026gt;value\u0026lt;/value\u0026gt; \u0026lt;/customValues\u0026gt; \u0026lt;/configuration\u0026gt; You can see that the nesting of the xml matches the string in the match element. If you need to choose between multiple elements on a specific node, the \u0026lt;element name\u0026gt;[@\u0026lt;identifying field\u0026gt;='string to match'] will allow you to do so. I.e. for the LOGENTRIES_TOKEN node, unless you specify this, you will not be able to distinguish between it and SomeRandomValue. To set a value between the xml tags, i.e. \u0026lt;myField\u0026gt;value\u0026lt;/myField\u0026gt;, you would use the matching string of match=\u0026quot;/configuration/customValues/value[@name='MyField']/text()\u0026quot;.\nIn later posts, I will be showing how to incorporate this into your PSake build script, here is what it will resemble:\nmsbuild /t:Package /v:q /p:VisualStudioVersion=12.0 /p:Configuration=Release /p:PackageLocation=..\\dist\\WebApi.Template.zip /p:ProjectParametersXMLFile=src\\WebApi.Template\\parameters.xml /p:DeployIisAppPath=dist src\\WebApi.Template\\WebApi.Template.csproj Lastly We now have the ability to build and package our project without including any configuration required to run it. When it is time to deploy the api, you can set values in the SetParameters.xml file according to your environment and use MSDeploy to deploy. At the moment, you might be thinking that is a lot of effort for something that still requires manual intervention. Stay tuned, in the next post, I will show you how to create a build script using PSake that will build on top of this and the following post how we start automating deployments using this mechanism.\n","date":"February 6, 2016","hero":"/posts/2016-02-06/parameterizing-webconfig/images/web_api_create_project.png","permalink":"http://localhost:1313/posts/2016-02-06/parameterizing-webconfig/","summary":"Most people would have experienced the issue of setting values in web.config for a project on different environments, i.e. the connection string for the database. My first attempt at resolving this was to simply create multiple configurations and build the appropriate one per environment. This has multiple issues: you are including sensitive information in your build artifact, creating different builds for the same version (to allow different values) and tightly coupling your build process to your environment values.","tags":["windows","webapi","builds"],"title":"Parameterizing Web.config"},{"categories":["aws"],"contents":"Ran into an issue where a Linux instance running on AWS in a private subnet was not updating the system time via NTP. First check was for the config file, but it had a list of servers, both inside and 1 outside AWS:\nserver 0.amazon.pool.ntp.org server 0.us.pool.ntp.org server 1.amazon.pool.ntp.org server 2.amazon.pool.ntp.org From this post (I would like to link to it, but it has been 8.5 years since I wrote this, and only found the missing link today on 2024/05/29) I tried both ntpdate and ntpdate-debian with the following results:\n$ ntpdate 16 Dec 22:24:14 ntpdate[32112]: no servers can be used, exiting $ ntpdate-debian 16 Dec 22:24:56 ntpdate[32117]: no servers can be used, exiting The instance did have access to a NAT box to allow access outside of the VPC and ntpq -p returned the following:\n$ ntpq -p remote refid st t when poll reach delay offset jitter ============================================================================== ranger.innolan. .INIT. 16 u - 64 0 0.000 0.000 0.000 4.53.160.75 .INIT. 16 u - 64 0 0.000 0.000 0.000 origin.towfowi. .INIT. 16 u - 64 0 0.000 0.000 0.000 ntp.your.org .INIT. 16 u - 64 0 0.000 0.000 0.000 Turns out that you need to explicitly allow udp/123 on the NAT instance\u0026rsquo;s Security Group. Once I added the rule, everything started working:\n$ ntpq -p remote refid st t when poll reach delay offset jitter ============================================================================== 216.152.240.220 216.218.254.202 2 u 1 64 1 37.486 11.927 0.138 ranger.innolan. 90.184.222.115 3 u 2 64 1 73.323 9.543 0.025 srcf-ntp.stanfo .shm0. 1 u 1 64 1 25.009 12.412 0.019 grom.polpo.org 127.67.113.92 2 u 2 64 1 22.254 13.909 0.000 ","date":"December 16, 2015","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2015-12-16/ntp-on-aws/","summary":"Ran into an issue where a Linux instance running on AWS in a private subnet was not updating the system time via NTP. First check was for the config file, but it had a list of servers, both inside and 1 outside AWS:\nserver 0.amazon.pool.ntp.org server 0.us.pool.ntp.org server 1.amazon.pool.ntp.org server 2.amazon.pool.ntp.org From this post (I would like to link to it, but it has been 8.5 years since I wrote this, and only found the missing link today on 2024/05/29) I tried both ntpdate and ntpdate-debian with the following results:","tags":["linux","aws","ntp"],"title":"NTP on AWS"},{"categories":["dotnet"],"contents":"I have been spending most of my time splitting up a monolithic system into separately deployable NuGet packages. The system is monolithic in the sense that there are multiple domains that are deployed as independent WCF services, but all reference the same set of base projects. This means that all the source sits in the same repository. The first step has been to move these common libraries into NuGet packages served from a local network folder and referencing them in each of these solutions as a package rather than project.\nThis is done fairly easily:\nCreate a new solution with a Class Library project in a new Git repo. Copy the classes from the old project to the new location and include in the project. Resolve any references - ReSharper for the system libs, NuGet restore for the others and then finally installing the newly created packages if the current package depends on it. (More on this later). Add a NuSpec file to the project. Build, package and push with NuGet. Also, if you aren\u0026rsquo;t using ReSharper yet, just give it a try - it will open your eyes. Or as they mentioned in the Coding Blocks podcast: \u0026ldquo;Using ReSharper on legacy code is like walking into your hotel room with a black light\u0026rdquo; - it will show you how bad it really is.\nBefore we get to the actual implementation, just a quick overview of the Git-Flow branching model:\nAll development is done on feature branches cut from develop and merged back in when done. When you have enough features for a release, you cut a short-lived release branch from develop. After this branch has been vetted, it will be merged into master and develop (to ensure any fixes are also merged back into you development stream). When merging into master, a tag with the new version number will be created. This will increment either the minor or major version number, depending on how big the release is. For hotfixes, you would branch from master directly, fix the issue and then merge it back into master as well as develop. Every time you merge into master, you update the version number and tag the source - this is the version that you want to use to stamp the releases with.\nThe command for the build \u0026amp; push with NuGet are (ignore the versioning and poking for further down):\ntask NuGetPackage -depends Init { $version = Generate-Semantic-Version-Number write-host \u0026#34;Building the package with version: [$version]\u0026#34; poke-xml $NuGetSpecFile \u0026#34;//e:id\u0026#34; $projectName @{\u0026#34;e\u0026#34; = \u0026#34;\u0026#34;} poke-xml $NuGetSpecFile \u0026#34;//e:version\u0026#34; $version @{\u0026#34;e\u0026#34; = \u0026#34;\u0026#34;} exec { \u0026amp; $NuGet pack $source_dir\\$projectName\\$projectName.csproj -NoPackageAnalysis -Build -Symbols -verbosity detailed -o $build_dir -Version $version -p Configuration=\u0026#34;release\u0026#34; } exec { \u0026amp; $NuGet push $build_dir\\$projectname.$version.nupkg -s \\\\networkserver\\NuGet } } This will result in the Class Library being packaged as a single NuGet package and pushed to the network share. This is the point where the PSake, GitFlow and Semantic Versions come in. When you use GitFlow, there is a very specific way to create new releases / hotfixes - this ties in perfectecly with SemVer. If you are just doing a minor feature release, you would increment the minor version number in the \u0026lt;major\u0026gt;.\u0026lt;minor\u0026gt;.\u0026lt;hotfix\u0026gt; version of the release. Same with major / hotfix numbers. The catch is that you want to also stamp your package with this version and ultimately enforce this via the build server to prevent accidental releases of incorrect versions. Part of the GitFlow release / hotfix workflow is to specify the tag name. Using the git command git describe --exact-match --abbrev=0 as part of the PSake function:\nfunction global:Generate-Semantic-Version-Number { $result = \u0026amp; \u0026#34;git\u0026#34; describe --exact-match --abbrev=0 | Out-String if ([string]::IsNullOrEmpty($result)) { $result = \u0026#34;1.0.0\u0026#34; } $result = $result -replace \u0026#34;`n\u0026#34;,\u0026#34;\u0026#34; -replace \u0026#34;`r\u0026#34;,\u0026#34;\u0026#34; return $result } You are able to extract the last tag on a branch that matches the current commit hash. When done on the master branch, you will always end up with the latest version - the only way for code to be merged into master in GitFlow is via a feature or hotfix finish. The build process that builds the library has write-access to the network share, ensuring that only builds that were created via the build server are pushed to your private NuGet repository.\nThe last part of the puzzle is to also use this version number in the Nuspec file and the various assemblies. Here are the handy PowerShell functions that I have run into that will update the assemblies:\nfunction global:Update-AssemblyInfoFiles ([string] $version) { $commonAssemblyInfo = \u0026#34;$source_dir\\CommonAssemblyInfo.cs\u0026#34; $assemblyDescriptionPattern = \u0026#39;AssemblyDescription\\(\u0026#34;(.*?)\u0026#34;\\)\u0026#39; $assemblyDescription = \u0026#39;AssemblyDescription(\u0026#34;\u0026#39; + $env:buildlabel + \u0026#39;\u0026#34;)\u0026#39;; $assemblyVersionPattern = \u0026#39;AssemblyVersion\\(\u0026#34;[0-9]+(\\.([0-9]+|\\*)){1,3}\u0026#34;\\)\u0026#39; $assemblyVersion = \u0026#39;AssemblyVersion(\u0026#34;\u0026#39; + $version + \u0026#39;\u0026#34;)\u0026#39;; $fileVersionPattern = \u0026#39;AssemblyFileVersion\\(\u0026#34;[0-9]+(\\.([0-9]+|\\*)){1,3}\u0026#34;\\)\u0026#39; $fileVersion = \u0026#39;AssemblyFileVersion(\u0026#34;\u0026#39; + $version + \u0026#39;\u0026#34;)\u0026#39;; Get-ChildItem $source_dir -r -filter AssemblyInfo.cs | ForEach-Object { $filename = $_.Directory.ToString() + \u0026#39;\\\u0026#39; + $_.Name $filename + \u0026#39; -\u0026gt; \u0026#39; + $version # If you are using a source control that requires to check-out files before # modifying them, make sure to check-out the file here. # For example, TFS will require the following command: # tf checkout $filename (Get-Content $commonAssemblyInfo) | ForEach-Object { % {$_ -replace $assemblyVersionPattern, $assemblyVersion } | % {$_ -replace $assemblyDescriptionPattern, $assemblyDescription } | % {$_ -replace $fileVersionPattern, $fileVersion } } | Set-Content $filename } } And the function to update the Nuspec xml file:\nfunction script:poke-xml($filePath, $xpath, $value, $namespaces = @{}) { [xml] $fileXml = Get-Content $filePath if($namespaces -ne $null -and $namespaces.Count -gt 0) { $ns = New-Object Xml.XmlNamespaceManager $fileXml.NameTable $namespaces.GetEnumerator() | %{ $ns.AddNamespace($_.Key,$_.Value) } $node = $fileXml.SelectSingleNode($xpath,$ns) } else { $node = $fileXml.SelectSingleNode($xpath) } if($node -eq $null) { return } if($node.NodeType -eq \u0026#34;Element\u0026#34;) { $node.InnerText = $value } else { $node.Value = $value } $fileXml.Save($filePath) } The actual xml update is done via:\n$version = Generate-Semantic-Version-Number write-host \u0026#34;Building the package with version: [$version]\u0026#34; poke-xml $NuGetSpecFile \u0026#34;//e:id\u0026#34; $projectName @{\u0026#34;e\u0026#34; = \u0026#34;\u0026#34;} poke-xml $NuGetSpecFile \u0026#34;//e:version\u0026#34; $version @{\u0026#34;e\u0026#34; = \u0026#34;\u0026#34;} Combining all of these together will allow you to release reliably versioned NuGet packages easily and the only dependency is PSake, PowerShell and git. This allows you to choose whatever flavour of build server you fancy. The obvious way to circumvent this is by committing directly to master and creating a tag by hand, but that is a human issue that the angry dev mob will sort out with their pitchforks.\n","date":"April 4, 2015","hero":"/posts/2015-04-04/gitflow-semver-psake/images/git_flow_release_cycle.png","permalink":"http://localhost:1313/posts/2015-04-04/gitflow-semver-psake/","summary":"I have been spending most of my time splitting up a monolithic system into separately deployable NuGet packages. The system is monolithic in the sense that there are multiple domains that are deployed as independent WCF services, but all reference the same set of base projects. This means that all the source sits in the same repository. The first step has been to move these common libraries into NuGet packages served from a local network folder and referencing them in each of these solutions as a package rather than project.","tags":["msbuild","psake","git","csharp"],"title":"Using the Git tag as a version number via PSake"},{"categories":["dotnet"],"contents":"Earlier today, I ran into the following error when trying to build a new WebApi solution using the command-line and MSBuild:\nThis is odd as v11 is Visual Studio 2012 and don\u0026rsquo;t even have 2012 installed. The issue manifests when you try to build the solution with: msbuild /t:Clean;Rebuild /v:q /p:Configuration=$projectConfig $source_dir\\$projectName.sln /p:Platform=\u0026quot;Any CPU\u0026quot;\nThe cause and solution is discussed on Stack Overflow - TL;DR version is this is due to adding the feature for older versions of Visual Studio, but then not setting the correct version number. To fix the issues, remove the following lines from your csproj file:\n\u0026lt;PropertyGroup\u0026gt; \u0026lt;VisualStudioVersion Condition=\u0026#34;\u0026#39;$(VisualStudioVersion)\u0026#39; == \u0026#39;\u0026#39;\u0026#34;\u0026gt;10.0\u0026lt;/VisualStudioVersion\u0026gt; \u0026lt;VSToolsPath Condition=\u0026#34;\u0026#39;$(VSToolsPath)\u0026#39; == \u0026#39;\u0026#39;\u0026#34;\u0026gt; $(MSBuildExtensionsPath32)\\Microsoft\\VisualStudio\\v$(VisualStudioVersion)\u0026lt;/VSToolsPath\u0026gt; \u0026lt;/PropertyGroup\u0026gt; After that, the build succeeds :)\nP.S.: Even changing the VisualStudioVersion to 12.0 does not work.\n","date":"March 30, 2015","hero":"/posts/2015-03-30/msbuild-external-error/images/MSB4019.png","permalink":"http://localhost:1313/posts/2015-03-30/msbuild-external-error/","summary":"Earlier today, I ran into the following error when trying to build a new WebApi solution using the command-line and MSBuild:\nThis is odd as v11 is Visual Studio 2012 and don\u0026rsquo;t even have 2012 installed. The issue manifests when you try to build the solution with: msbuild /t:Clean;Rebuild /v:q /p:Configuration=$projectConfig $source_dir\\$projectName.sln /p:Platform=\u0026quot;Any CPU\u0026quot;\nThe cause and solution is discussed on Stack Overflow - TL;DR version is this is due to adding the feature for older versions of Visual Studio, but then not setting the correct version number.","tags":["msbuild","errors","visualstudio","csharp"],"title":"External MSBuild for WebApi in VS 2013 fails"},{"categories":["Version Control"],"contents":"One of those djissis duh moments - trying to check out a public git repo, but failing due to authentication \u0026hellip;\ngit clone git@github.com:cobusbernard/Scripts.git Cloning into \u0026#39;Scripts\u0026#39;... Warning: Permanently added the RSA host key for IP address \u0026#39;192.30.252.131\u0026#39; to the list of known hosts. Permission denied (publickey). fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. So I checked my ssh setup / keys, but nothing in there looks dodgy:\nls ~/ssh/ known_hosts So it ended up being ssh vs https cloning. SSH checkout is always authenticated.\ngit clone https://github.com/cobusbernard/Scripts.git Cloning into \u0026#39;Scripts\u0026#39;... remote: Counting objects: 9, done. remote: Compressing objects: 100% (3/3), done. remote: Total 9 (delta 0), reused 0 (delta 0), pack-reused 6 Unpacking objects: 100% (9/9), done. Checking connectivity... done. ","date":"February 27, 2015","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2015-02-27/git-checkout-ssh-vs-https/","summary":"One of those djissis duh moments - trying to check out a public git repo, but failing due to authentication \u0026hellip;\ngit clone git@github.com:cobusbernard/Scripts.git Cloning into \u0026#39;Scripts\u0026#39;... Warning: Permanently added the RSA host key for IP address \u0026#39;192.30.252.131\u0026#39; to the list of known hosts. Permission denied (publickey). fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. So I checked my ssh setup / keys, but nothing in there looks dodgy:","tags":["ssh","https","git"],"title":"Git checkout error for public repo"},{"categories":["Linux"],"contents":"Update 2016-01-09: In the previous version, I never added the command for creating the array and since then I needed it for a new server. When the array starts rebuilding, it will look like there is a failed drive: this is normal. When it builds from scratch, it is actually doing the rebuild process and will indicate a failed drive until the array is rebuilt.\n# mdadm --create --verbose /dev/md0 --level=5 --raid-devices=3 /dev/sdb1 /dev/sdc1 /dev/sdd1 # cat /proc/mdstat md0 : active raid5 sdd1[3] sdc1[1] sdb1[0] 4294702080 blocks super 1.2 level 5, 512k chunk, algorithm 2 [3/2] [UU_] [====\u0026gt;................] recovery = 20.1% (433131624/2147351040) finish=420.1min speed=68006K/sec As you can see from the above, this isn\u0026rsquo;t very fast. I found this article with some tips on increasing the speed. The one that worked for me was increasing the stripe-cache_size (tip 3):\n# echo 32768 \u0026gt; /sys/block/md0/md/stripe_cache_size md0 : active raid5 sdd1[3] sdc1[1] sdb1[0] 4294702080 blocks super 1.2 level 5, 512k chunk, algorithm 2 [3/2] [UU_] [====\u0026gt;................] recovery = 24.3% (523330980/2147351040) finish=147.1min speed=183976K/sec Much better, about 3x faster.\nFinding the broken drive Original With all the load shedding, a drive failed in my software raid array. It is a Raid-5 setup, so I had time to replace the drive and rebuild the array. I was lucky with this failure as I randomly looked at my mdadm array today - I really need to set up the mail details to allow the system to mail me when something fails. Here is the command to check the health of your arrays:\ncat /proc/mdstat Personalities : [raid6] [raid5] [raid4] [linear] [multipath] [raid0] [raid1] [raid10] md0 : active raid5 sdc1[2] sda1[0] sdb1[1] 2929890816 blocks super 1.2 level 5, 512k chunk, algorithm 2 [4/3] [UUU_] unused devices: \u0026lt;none\u0026gt; Replacing the broken drive The [UUU_] indicates that one of the drives is no longer part of the array. To see which drive is faulty, run mdadm --detail /dev/md0 to get the details:\n/dev/md0: Version : 1.2 Creation Time : Mon Jun 23 20:08:17 2014 Raid Level : raid5 Array Size : 2929890816 (2794.16 GiB 3000.21 GB) Used Dev Size : 976630272 (931.39 GiB 1000.07 GB) Raid Devices : 4 Total Devices : 3 Persistence : Superblock is persistent Update Time : Fri Feb 17 08:24:12 2015 State : clean, degraded Active Devices : 3 Working Devices : 3 Failed Devices : 0 Spare Devices : 0 Layout : left-symmetric Chunk Size : 512K Name : Token:0 (local to host Token) UUID : 0b9ec9a1:73b38783:7bccad64:04b10ed8 Events : 758663 Number Major Minor RaidDevice State 0 8 1 0 active sync /dev/sda1 1 8 17 1 active sync /dev/sdb1 2 8 33 2 active sync /dev/sdc1 4 0 0 3 removed This indicated that the last drive /dev/sdd1 was missing. To figure out which drive this is, you can grab the serial number using SmartMontools\nsudo apt-get install smartmontools smartctl -a /dev/sdd | grep Serial Serial Number: 3NX12345 Time to open up the server and find the faulty drive. My server has hot swap bays and I guessed \u0026lsquo;sdd\u0026rsquo; to be the fourth drive, so got the right one on my first attempt. Replaced the drive with a spare I had - originally I ran a 8x 1TB Raid-5 array on a different server, but the power usage was too high, so scaled things down a bit. To get the backup drive correctly partitioned, I deleted all the existing partitions and added a new one with file-system type \u0026lsquo;fd\u0026rsquo; for \u0026lsquo;Linux Raid autodetect\u0026rsquo; using fdisk. To add the disk back into the array, use mdadm --manage /dev/md0 --add /dev/sdd1. To check the progress, look at /proc/mdstat:\ncat /proc/mdstat Personalities : [raid6] [raid5] [raid4] [linear] [multipath] [raid0] [raid1] [raid10] md0 : active raid5 sdd1[4] sdc1[2] sda1[0] sdb1[1] 2929890816 blocks super 1.2 level 5, 512k chunk, algorithm 2 [4/3] [UUU_] [\u0026gt;....................] recovery = 0.0% (190080/976630272) finish=256.8min speed=63360K/sec unused devices: \u0026lt;none\u0026gt; All good, now just need to wait till the rebuild is complete. It is advisable to stop all processes / applications during this period to speed up the rebuild. In my case, stopping XMBC increased the speed by 10x. Once the rebuild is complete, you should see something like this:\ncat /proc/mdstat Personalities : [raid6] [raid5] [raid4] [linear] [multipath] [raid0] [raid1] [raid10] md0 : active raid5 sdd1[4] sdc1[2] sda1[0] sdb1[1] 2929890816 blocks super 1.2 level 5, 512k chunk, algorithm 2 [4/4] [UUUU] unused devices: \u0026lt;none\u0026gt; Notifications for future failures Now to ensure we get notified about any future fails, we need to set up mailing. I found this post which looked to be what I needed. Here is the short version:\nsudo apt-get install msmtp msmtp-mta To find the global configuration file, use msmtp --version | grep \u0026quot;System configuration\u0026quot;:\nSystem configuration file name: /etc/msmtprc Edit the file with sudo vi /etc/msmtprc and add (remember to replace with your specific details and also to enable SMTP on your GMail account):\n# ------------------------------------------------------------------------------ # msmtp System Wide Configuration file # ------------------------------------------------------------------------------ # A system wide configuration is optional. # If it exists, it usually defines a default account. # This allows msmtp to be used like /usr/sbin/sendmail. # ------------------------------------------------------------------------------ # Accounts # ------------------------------------------------------------------------------ # gmail account # Configuring for gmail is beyond the scope of this tutorial # Googling for \u0026#34;gmail msmtp\u0026#34; should help account gmail host smtp.gmail.com port 587 from youremail@gmail.com tls on tls_starttls on tls_trust_file /etc/ssl/certs/ca-certificates.crt auth on user youremail password supersecret syslog LOG_MAIL # Other ISP Account # Configuring for other ISPs is beyond the scope of this tutorial # Googling for \u0026#34;myisp outlook smtp\u0026#34; should help # ------------------------------------------------------------------------------ # Configurations # ------------------------------------------------------------------------------ # Construct envelope-from addresses of the form \u0026#34;user@oursite.example\u0026#34;. #auto_from on #maildomain fermmy.server # Use TLS. #tls on #tls_trust_file /etc/ssl/certs/ca-certificates.crt # Syslog logging with facility LOG_MAIL instead of the default LOG_USER. # Must be done within \u0026#34;account\u0026#34; sub-section above #syslog LOG_MAIL # Set a default account account default : gmail # ------------------------------------------------------------------------------ To test, run echo \u0026quot;This is a test e-mail from my server using msmtp\u0026quot; | msmtp -d youremail@gmail.com. It should print out multiple lines, any error will be clearly displayed as such, i.e.\n--\u0026gt; AUTH PLAIN AGNvYnVzYXyXbmFyZGuYYmthdnFiam9seXZlaGJiZA== \u0026lt;-- 535-5.7.8 Username and Password not accepted. Learn more at \u0026lt;-- 535 5.7.8 http://support.google.com/mail/bin/answer.py?answer=14257 n1sm2519061wib.11 - gsmtp msmtp: authentication failed (method PLAIN) msmtp: server message: 535-5.7.8 Username and Password not accepted. Learn more at msmtp: server message: 535 5.7.8 http://support.google.com/mail/bin/answer.py?answer=14257 n1sm2519061wib.11 - gsmtp msmtp: could not send mail (account default from /etc/msmtprc) Now to replace the sendmail command with msmtp:\nsudo mv /usr/sbin/sendmail /usr/sbin/sendmail.bak sudo ln -s /usr/bin/msmtp /usr/sbin/sendmail Add these lines to you /etc/mdadm/mdadm.conf file:\n# instruct the monitoring daemon where to send mail alerts MAILADDR youremail@domain.com MAILFROM my-server-name - mdadm Then to test, use sudo mdadm --monitor --scan --test --oneshot. If all is set up correctly, you should get a mail like this:\nThis is an automatically generated mail message from mdadm running on Token A SparesMissing event had been detected on md device /dev/md0. Faithfully yours, etc. P.S. The /proc/mdstat file currently contains the following: Personalities : [raid6] [raid5] [raid4] [linear] [multipath] [raid0] [raid1] [raid10] md0 : active raid5 sdc1[2] sdb1[1] sda1[0] sdd1[4] 2929890816 blocks super 1.2 level 5, 512k chunk, algorithm 2 [4/4] [UUUU] unused devices: \u0026lt;none\u0026gt; So after all that, you should be notified when something goes wrong and be able to correct it before loosing all your data.\n","date":"February 19, 2015","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2015-02-19/mdadm-replace-drive/","summary":"Update 2016-01-09: In the previous version, I never added the command for creating the array and since then I needed it for a new server. When the array starts rebuilding, it will look like there is a failed drive: this is normal. When it builds from scratch, it is actually doing the rebuild process and will indicate a failed drive until the array is rebuilt.\n# mdadm --create --verbose /dev/md0 --level=5 --raid-devices=3 /dev/sdb1 /dev/sdc1 /dev/sdd1 # cat /proc/mdstat md0 : active raid5 sdd1[3] sdc1[1] sdb1[0] 4294702080 blocks super 1.","tags":["linux","mdadm"],"title":"Replacing mdadm drive"},{"categories":["Linux"],"contents":"In my previous post, I set up OpenVPN on my home network and everything was awesome. Until this morning: I could not connect to my VPN. I had forgotten to set up some kind of dynamic DNS updater for it. That should be easy enough, I had previously done this using DynDNS. Only problem was that the service is no longer free. This shouldn\u0026rsquo;t be too much of a problem as I have a couple of my own domains - yes, I will one day still get round to finishing my pet project \u0026lsquo;Tinkle Tones\u0026rsquo; ;)\nMy domains are split between a couple of different registrars using various tools to manage the DNS updating. Almost none of them had any kind of API for updating the DNS and I really don\u0026rsquo;t want to create a hack by doing screen scraping and form posting. I decided to use Route 53 from Amazon to allow easy scripting of all my DNS needs. The last time I played with the service was in 2011 while doing some infrastructure automation / setup for 22seven. Quite a lot has changed in terms of features / the interface and I was pleasantly surprised to be able to register domains as well. The prices seemed on par with my registrars\u0026rsquo; fees; variation was a couple of dollars, but I would pay that with a smile if it allowed easy management. My only gripe was the cost of transferring a domain: it looks to be the same cost of registering a new one. A couple of my domains have just renewed, so I am going to wait before moving them.\nCreating a new domain is dead easy: click \u0026lsquo;Created Hosted Domain\u0026rsquo;, add the domain name, a comment if you want and hit \u0026lsquo;Create\u0026rsquo;. The resulting screen provides you with 4 name servers scattered across the world, I had a a .co.uk, .com, .net and .org. The numbers make you realise just how many nameservers Amazon has - my largest one was 1815. After updating my NS records at my registrar and duplicating the existing records on Route 53, I was ready to go.\nSomething for my TODO list: create a script for the Google Apps MX records as I will use this for all my other domains - I am lucky enough to still have a couple of the free accounts.\nI found this script by searching for \u0026lsquo;route 53 script update dns\u0026rsquo;. Looks to be what I need, but the https cert was invalid for a short amount of time. Solution was to add another parameter to the curl command \u0026lsquo;-k\u0026rsquo; - credit to this post.\nThe script was a good start, but I wanted to externalize the sensitive parts to allow committing this to a public repository. This led to a merry chase to acquire answers to more questions - Aarron Patterson spoke about The joy of programming at RubyFuza recently. Moving along swiftly \u0026hellip;\nTo interact with the AWS API, you will need to install the CLI tools. Quick notes copied from there to get pip installed:\ncd /tmp curl \u0026#34;https://bootstrap.pypa.io/get-pip.py\u0026#34; -o \u0026#34;get-pip.py\u0026#34; sudo python get-pip.py pip --help And then the CLI:\nsudo pip install awscli Before we can use the CLI, we will need credentials to interact with the AWS. To generate these, open up your AWS console in the browser and go to the IAM users. Create a new user with the following security policy - copy Hosted Zone ID from Route 53 and replace the value in the ARN below with your one:\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:ChangeResourceRecordSets\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:route53:::hostedzone/BJBK35SKMM9OE\u0026#34;\r]\r}\r]\r} After creating the user, you will be presented with a AWS access key and secret for this user - keep them safe somewhere are you cannot retrive them after this point.\nTo set up an AWS CLI profile, use the credentials provided in the previous step, the default region and output format can be left empty:\naws configure --profile dns-update-your-site Finally, to test that your script is working, run it:\n./update-route53.sh BJBK35SKMM9OE dns-update-your-site example.com 1 This should output the following:\nForce update is set.\rIP has changed to 10.0.0.1, updating ... The final step is to add this to a cron job, by running crontab -e and adding in (note that the trailing 1 has been removed to not force an update unless the IP has changed):\n*/30 * * * * /home/will/scripts/update-route53.sh BJBK35SKMM9OE dns-update-your-site example.com You should now have an updating DNS for your home connection that will update when you external IP changes by calling into Amazon\u0026rsquo;s Route 53 API.\n","date":"February 17, 2015","hero":"/posts/2015-02-17/route-53-dns-update-script/images/route.53.dns.png","permalink":"http://localhost:1313/posts/2015-02-17/route-53-dns-update-script/","summary":"In my previous post, I set up OpenVPN on my home network and everything was awesome. Until this morning: I could not connect to my VPN. I had forgotten to set up some kind of dynamic DNS updater for it. That should be easy enough, I had previously done this using DynDNS. Only problem was that the service is no longer free. This shouldn\u0026rsquo;t be too much of a problem as I have a couple of my own domains - yes, I will one day still get round to finishing my pet project \u0026lsquo;Tinkle Tones\u0026rsquo; ;)","tags":["linux","openvpn","vpn"],"title":"Moving DNS to Amazon Route 53, Dynamic Updates"},{"categories":["Linux"],"contents":"Having just set up an external VPN to pretend to be in the US, I wanted one to get into my home network as well when on the road. I ran into the issue while in the USA recently where I was unable to purchase Steam games as their servers picked up that I was in the States, but my payment method was South African. Never thought I would need to spoof my own IP Address to pretend to be in South Africa.\nThe installation was almost exactly the same with one exception - I needed to install AppArmour as it wasn\u0026rsquo;t part of my system at that point. My little HP N40L that serves as a home file / XBMC server started out as an Ubuntu 12.04 server and I needed to do a dist-upgrade on it. For a system that has been running a software, 4-disk RAID-5 using MDADM for a few years, I was a bit worried that something would break. But it is just a server that I play with, worst case I would need to redo it. Eyes closed, fingers crossed, I started with the upgrade:\nsudo apt-get update sudo apt-get upgrade sudo apt-get dist-upgrade sudo apt-get install apparmor After a while (remember, this is Africa where we have really slow bandwidth - you can find us at #122), the upgrade was done. A reboot later, and all seemed to still be working. OpenVPN installation took longer than yesterday, but only due to the bandwidth constraints. Only thing remaining was to expose the udp/1194 port to the external world. I have a RouterBoard 750GL that I use for my firewall at home, a single command sorted this out for me:\n/ip firewall nat add chain=dstnat protocol=udp dst-port=1194 \\ action=dst-nat to-addresses=10.0.0.1 to-ports=1194 in-interface=uncapped ","date":"February 14, 2015","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2015-02-14/openvpn-home/","summary":"Having just set up an external VPN to pretend to be in the US, I wanted one to get into my home network as well when on the road. I ran into the issue while in the USA recently where I was unable to purchase Steam games as their servers picked up that I was in the States, but my payment method was South African. Never thought I would need to spoof my own IP Address to pretend to be in South Africa.","tags":["linux","openvpn","vpn","docker"],"title":"Home VPN with OpenVPN"},{"categories":["Linux"],"contents":"I was playing around on a random site today, when I received the message This content is not available in your country yet. As a South African, we run into this a lot as our online presence it not that large and most international companies do not feel it is financially viable to run their services here. So I decided it is time to get a VPN going to avoid this. I decided to use a Digital Ocean Droplet for the task - costing $5 a month is cheap enough. The smallest one has 512MB Ram, 20GB SSD, 1 core and 1TB of transfer - this will be plenty to get the VPN up and possibly some other services. Signup was very quick and I was happy to see that they support 2 factor auth via Google Authenticator.\nI found this easy guide on how to set up OpenVPN] using Docker - blindly followed it and after 5 minutes, I could log on to the new VPN. This is a lot quicker than my first attempts in 2011 - I would first set up a VM with Ubuntu, update it and then do all the installation / configuration of OpenVPN manually. Sometimes you would run into some fun issues when dealing with Ubuntu on Hyper-V.\nHere are the commands in order for quick copy \u0026amp; paste - still need to create or find an Ansible playbook for this to make it even easier. The commands should be run as a normal user - it will sudo where needed.\nDOMAIN=vpn.yourdomain.com CURRENT_USER=$(whoami) OVPN_DATA=\u0026#34;ovpn-data\u0026#34; DOCKER_CONFIG=/etc/init/docker-openvpn.conf curl https://get.docker.io/gpg | sudo apt-key add - echo deb http://get.docker.io/ubuntu docker main | sudo tee /etc/apt/sources.list.d/docker.list sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y lxc-docker sudo usermod -aG docker $CURRENT_USER su - $CURRENT_USER docker run --name $OVPN_DATA -v /etc/openvpn busybox docker run --volumes-from $OVPN_DATA --rm kylemanna/openvpn ovpn_genconfig -u udp://$DOMAIN:1194 docker run --volumes-from $OVPN_DATA --rm -it kylemanna/openvpn ovpn_initpki sudo touch $DOCKER_CONFIG sudo printf \u0026#39;%s\\n%s\\n%s\\n%s\\n%s\\n%s\\n%s\\n\u0026#39; \\ \u0026#39;description \u0026#34;Docker container for OpenVPN server\u0026#34;\u0026#39; \\ \u0026#39;start on filesystem and started docker\u0026#39; \\ \u0026#39;stop on runlevel [!2345]\u0026#39; \\ \u0026#39;respawn\u0026#39; \\ \u0026#39;script\u0026#39; \\ \u0026#39; exec docker run --volumes-from ovpn-data --rm -p 1194:1194/udp --cap-add=NET_ADMIN kylemanna/openvpn\u0026#39; \\ \u0026#39;end script\u0026#39; \u0026gt;\u0026gt; $DOCKER_CONFIG sudo start docker-openvpn ```~ At this point, the Docker container will be created, configured and running via an upstart script. To add a user, use the following 2 commands - first one will create the user and the second will output the config file to the host VM to allow copying it to the client. ```bash docker run --volumes-from $OVPN_DATA --rm -it kylemanna/openvpn easyrsa build-client-full CLIENTNAME nopass docker run --volumes-from $OVPN_DATA --rm kylemanna/openvpn ovpn_getclient CLIENTNAME \u0026gt; CLIENTNAME.ovpn The client isntallation is trivial and the guide does a great job covering all the major operating systems.\n","date":"February 13, 2015","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2015-02-13/openvpn-on-digital-ocean/","summary":"I was playing around on a random site today, when I received the message This content is not available in your country yet. As a South African, we run into this a lot as our online presence it not that large and most international companies do not feel it is financially viable to run their services here. So I decided it is time to get a VPN going to avoid this.","tags":["linux","openvpn","vpn","docker"],"title":"Personal VPN - OpenVPN in Docker on DigitalOcean"},{"categories":["Windows"],"contents":"I was playing with Docker yesterday and needed to convert a key that I had been using in Windows with Putty for a while. First attempt with puttygen didn\u0026rsquo;t work at all, then I found this Stack Overflow post, copying the details here so I know where to find it next time :)\nTo install the required tools, use\nUbuntu: sudo apt-get install putty-tools OSX: brew install putty After the required tools have been installed, use the following 2 commands to extract the private and public portion of the key:\nputtygen id_dsa.ppk -O private-openssh -o id_dsa puttygen id_dsa.ppk -O public-openssh -o id_dsa.pub ","date":"February 8, 2015","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2015-02-08/convert-putty-keys-to-openssh/","summary":"I was playing with Docker yesterday and needed to convert a key that I had been using in Windows with Putty for a while. First attempt with puttygen didn\u0026rsquo;t work at all, then I found this Stack Overflow post, copying the details here so I know where to find it next time :)\nTo install the required tools, use\nUbuntu: sudo apt-get install putty-tools OSX: brew install putty After the required tools have been installed, use the following 2 commands to extract the private and public portion of the key:","tags":["ssh","putty"],"title":"Converting Putty keys to OpenSSH"},{"categories":["Ruby"],"contents":"So I finally got round to sorting out the tech blog. I chose to use Jekyll rather than WordPress as I didn\u0026rsquo;t want to deal with constant updates, security vulnerabilities and backing up of the site. Jekyll generates a static site based on the posts created, but still has the advantages of a CMS like tags and categories. Posts are written using Markdown in a plain text file, making it very easy to do the formatting. You are able to generate the site locally before publishing / pushing it elsewhere. This is where one of the other benefits come in: GitHub Pages. You are able to create a repository with the format \u0026lt;GitHub Username\u0026gt;.github.io - in my case, that is cobusbernard.github.io. GitHub itself uses Jekyll, and by creating this repository, it automatically creates the Jekyll site for you. No backup is required as your site is already under version control in GitHub!\nGetting Started First things first, need to have a decent look to the blog. Easiest solution was to grab a theme from Jekyll Themes, I decided to go for dbyll that you can preview here. After editing the required configs (hint: _config.yml), I previewed the outcome by running:\njekyll serve High-fives all round\u0026hellip; Celebrating mediocrity is fun ;) Moving along swiftly\u0026hellip;\nPublishing to GitHub To be able to publish this as a GitHub Pages project, you will first need to create a repository in GitHub called \u0026lt;GitHub Username\u0026gt;.github.io. The username has to match exactly what you have as your username in GitHub, mine is \u0026lsquo;cobusbernard\u0026rsquo;. After creating the repo, you will need to initialise the local Jekyll folder as a git repo, set the remote to point to the newly created GitHub project and push your first version:\ngit init git remote set-url origin git@github.com:cobusbernard/cobusbernard.github.io.git git push -u origin master Viola! Head on over to \u0026lt;GitHub Username\u0026gt;.github.io and you should see your site. Most of the themes ship with some sample posts in the _posts folder. I created a _drafts folder to keep these for reference while I get comfortable with the Markdown syntax.\nCustom Domain I recently bought the domain cobus.io and wanted to point that to this blog. The GitHub instructions seemed simple enough. To enable this, first add a file to your repo called CNAME (all-caps is important!) in the root of your repository with the domain name as content:\ncobus.io Do not prefix with http://!\nGo to your GitHub repo and look at the settings section - to do this, follow these instructions. GitHub -\u0026gt; Repository -\u0026gt; Settings (right sidebar) -\u0026gt; Scroll down to GitHub Pages. You should see your custom domain name with a checkmark and a green background if all went well.\nLastly, you need to point your domain to the GitHub site by adding a CNAME or a custom apex domain. In my case, I used the custom apex domain as I wanted just cobus.io to be the URL for the blog and not something like blog.cobus.io. This required having two A records (changing the existing one and adding another) for the GitHub IPs:\n192.30.252.153 192.30.252.154 Once the DNS change has propogated (up to 24h), you will be able to access your blog / site from \u0026lt;GitHub Username\u0026gt;.github.io.\nThat is it, you should now be ready to start adding some content.\nAdding Content Now let\u0026rsquo;s try out some code blocks. Currently I am doing most of my work in C#, so would like to see if Jekyll supports the text markup for this correctly:\npublic void Main(string args[]) { System.Console.Writeln(\u0026#34;Hello World\u0026#34;); } That worked! The markup is very easy, just wrap the code in {% raw %} {% highlight csharp %} {% endraw %} tags, i.e.\n{% raw %} {% highlight csharp %} public void Main(string args[]) { System.Console.Writeln(\u0026#34;Hello World\u0026#34;); } {% endhighlight %} {% endraw %} Google Analytics This will be a completely new area for me as I know what analytics do, but have never worked with them before. The easiest way to get going is to create an account on Google Analytics, set up a site and stick the chunk of Javascript they give you into the page. To do this cleanly, create a new file in _includes/google_analytics.html that will contain the following:\n\u0026lt;script\u0026gt; (function(i,s,o,g,r,a,m){i[\u0026#39;GoogleAnalyticsObject\u0026#39;]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\u0026#39;script\u0026#39;,\u0026#39;//www.google-analytics.com/analytics.js\u0026#39;,\u0026#39;ga\u0026#39;); ga(\u0026#39;create\u0026#39;, \u0026#39;\u0026lt;tracking code\u0026gt;\u0026#39;, \u0026#39;\u0026lt;site name\u0026gt;\u0026#39;); ga(\u0026#39;send\u0026#39;, \u0026#39;pageview\u0026#39;); \u0026lt;/script\u0026gt; This needs to be added to all the pages by adding the following line to _includes/default.html:\n{% include google_analytics.html %} Comments One of the major pain points for me on my almost non-existent blog were the number of spam comments I had to delete. I installed some plugins to deal with it, but some still slipped through. There were even some cases where my WordPress site was high-jacked due to some weird comment being added. To avoid this, use Disqus - create an account and set up your first site. Choose the Universal code option at the end and they will provide you with a chunk of Javascript to paste in. As with the Google Analytics, we don\u0026rsquo;t want to just dump this into the file, but rather use another include. Create a file _includes/comments.html and add the Javascript into it:\n\u0026lt;div id=\u0026#34;disqus_thread\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */ var disqus_shortname = \u0026#39;\u0026lt;disqus forum name\u0026gt;\u0026#39;; // required: replace example with your forum shortname /* * * DON\u0026#39;T EDIT BELOW THIS LINE * * */ (function() { var dsq = document.createElement(\u0026#39;script\u0026#39;); dsq.type = \u0026#39;text/javascript\u0026#39;; dsq.async = true; dsq.src = \u0026#39;//\u0026#39; + disqus_shortname + \u0026#39;.disqus.com/embed.js\u0026#39;; (document.getElementsByTagName(\u0026#39;head\u0026#39;)[0] || document.getElementsByTagName(\u0026#39;body\u0026#39;)[0]).appendChild(dsq); })(); \u0026lt;/script\u0026gt; \u0026lt;noscript\u0026gt;Please enable JavaScript to view the \u0026lt;a href=\u0026#34;https://disqus.com/?ref_noscript\u0026#34;\u0026gt;comments powered by Disqus.\u0026lt;/a\u0026gt;\u0026lt;/noscript\u0026gt; To add the this to the page layout, edit the default.html page to include it above the footer by adding in:\ninclude comments.html This will display the comments on all pages. To add the ability to exclude comments from certain posts, add a variable at the top of the specific post with:\nexclude_comments: true Also wrap the _comments.html content with:\n{% raw %} {% if page.exclude_comments == false %} {% endif %} {% endraw %} When exclude_comments is set to true, the Disqus comment section will not be displayed.\n","date":"January 26, 2015","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2015-01-26/basic-github-jekyll-blog/","summary":"So I finally got round to sorting out the tech blog. I chose to use Jekyll rather than WordPress as I didn\u0026rsquo;t want to deal with constant updates, security vulnerabilities and backing up of the site. Jekyll generates a static site based on the posts created, but still has the advantages of a CMS like tags and categories. Posts are written using Markdown in a plain text file, making it very easy to do the formatting.","tags":["demo","jekyll","setup","ruby"],"title":"Basic Jekyll GitHub Blog"},{"categories":["Linux"],"contents":"Everyone who has an SSH port open to the world knows the amount of brute force attempts you will get. It doesn’t matter if you only accept keys, the script kiddies will still try. Easiest fix for this is rate limiting: you can only attempt to login 3 times per 10 minutes. This does not include successful logins, only failed ones. To do this, use the following IPTables commands:\nsudo iptables -A INPUT -p tcp --dport 22 -m state --state NEW -m recent --set --name SSH --rsource sudo iptables -A INPUT -m recent --update --seconds 600 --hitcount 4 --rttl --name SSH --rsource -j DROP sudo iptables-save \u0026gt; /etc/iptables_rules Add the following to /etc/rc.local before the ‘exit':\n/sbin/iptables-restore \u0026lt; /etc/iptables_rules This will add the rules back if you reboot.\n","date":"January 28, 2013","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2013-01-28/ssh-rate-limit/","summary":"Everyone who has an SSH port open to the world knows the amount of brute force attempts you will get. It doesn’t matter if you only accept keys, the script kiddies will still try. Easiest fix for this is rate limiting: you can only attempt to login 3 times per 10 minutes. This does not include successful logins, only failed ones. To do this, use the following IPTables commands:\nsudo iptables -A INPUT -p tcp --dport 22 -m state --state NEW -m recent --set --name SSH --rsource sudo iptables -A INPUT -m recent --update --seconds 600 --hitcount 4 --rttl --name SSH --rsource -j DROP sudo iptables-save \u0026gt; /etc/iptables_rules Add the following to /etc/rc.","tags":["linux","security","ssh"],"title":"Linux SSH rate limiting"},{"categories":["Linux"],"contents":"If you use SSH on Linux a lot, you will know that having to specify the key each time you want to access a server is a pain. You really don\u0026rsquo;t want to have a single key for all servers as you would need to revoke it globally in the event that it was compromised. The ssh config file allows you to specify multiple values to use for specific hosts:\nTo edit the ssh config, open it in vim (or your favourite editor):\nvi ~/.ssh/config Add the following to it:\nHost {personalaccount}.unfuddle.com IdentityFile ~/.ssh/id_rsa IdentitiesOnly yes User {personaluser} Host {companyaccount}.unfuddle.com IdentityFile ~/.ssh/{companyaccount}_rsa IdentitiesOnly yes User {mycompanyuser} This will allow you to use different keys depending on if you are accessing the server using your personal or company user. This becomes very useful when dealing with multiple repositories hosted on the same service.\n","date":"April 12, 2012","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2012-04-12/ssh-config-multiple-keys/","summary":"If you use SSH on Linux a lot, you will know that having to specify the key each time you want to access a server is a pain. You really don\u0026rsquo;t want to have a single key for all servers as you would need to revoke it globally in the event that it was compromised. The ssh config file allows you to specify multiple values to use for specific hosts:","tags":["config","ssh","linux"],"title":"Using SSH Config"},{"categories":["Linux"],"contents":"If you need to set Chrome as the default browser in Xubuntu, run the following from your terminal:\ngconftool-2 --type string -s /desktop/gnome/url-handlers/http/command \u0026#34;google-chrome %s\u0026#34; ","date":"April 10, 2012","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2012-04-10/chrome-default-xubuntu/","summary":"If you need to set Chrome as the default browser in Xubuntu, run the following from your terminal:\ngconftool-2 --type string -s /desktop/gnome/url-handlers/http/command \u0026#34;google-chrome %s\u0026#34; ","tags":["linux","config","xubuntu"],"title":"Chrome as default browser in Xubuntu"},{"categories":["Linux"],"contents":"To install the Java JDK using only the terminal, follow the following steps:\nDownload the JDK from:\nhttp://www.oracle.com/technetwork/java/javase/downloads/java-se-jdk-7-download-432154.html Then run the following commands (change the filename as required):\ntar -xvf jdk-7-linux-i586.tar.gz sudo mv ./jdk1.7.0 /usr/lib/jvm/jdk1.7.0 sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk1.7.0/jre/bin/java 1 sudo update-alternatives --config java sudo update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/jdk1.7.0/bin/javac 1 sudo update-alternatives --config javac To test that everything is working:\njava -version Should output:\njava version \u0026#34;1.7.0\u0026#34; Java(TM) SE Runtime Environment (build 1.7.0-b147) Java HotSpot(TM) 64-Bit Server VM (build 21.0-b17, mixed mode) javac -version Should output:\njavac 1.7.0 ","date":"March 27, 2012","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2012-03-27/terminal-install-java-ubuntu-10.10/","summary":"To install the Java JDK using only the terminal, follow the following steps:\nDownload the JDK from:\nhttp://www.oracle.com/technetwork/java/javase/downloads/java-se-jdk-7-download-432154.html Then run the following commands (change the filename as required):\ntar -xvf jdk-7-linux-i586.tar.gz sudo mv ./jdk1.7.0 /usr/lib/jvm/jdk1.7.0 sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk1.7.0/jre/bin/java 1 sudo update-alternatives --config java sudo update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/jdk1.7.0/bin/javac 1 sudo update-alternatives --config javac To test that everything is working:\njava -version Should output:\njava version \u0026#34;1.7.0\u0026#34; Java(TM) SE Runtime Environment (build 1.","tags":["java","linux"],"title":"Install Java JDK using terminal"},{"categories":["Version Control"],"contents":"While I was working on Dementium II HD, I wanted to migrate the repository from SVN to Git. It was being hosted on a machine in the one studio and served the external developers over a 10mbit ADSL connection. The upstream rate is ~400kbit, so imagine trying to checkout a 26GB repository. I ran into the following error when I tried to import the SVN repo into a Git one:\n$ git push -v origin svn-merge-trunk:svn-trunk Pushing to https://git-repo/git Counting objects: 2840, done. Delta compression using up to 4 threads. Compressing objects: 100% (2788/2788), done. error: RPC failed; result=22, HTTP code = 411 fatal: The remote end hung up unexpectedly Writing objects: 100% (2840/2840), 3.07 MiB | 474 KiB/s, done. Total 2840 (delta 2202), reused 0 (delta 0) fatal: The remote end hung up unexpectedly After a bit of Googling, I found a solution for this:\n$ git config http.postBuffer 524288000 $ git push -v origin master:master Pushing to https://git-repo/git Username: Password: Counting objects: 2843, done. Delta compression using up to 4 threads. Compressing objects: 100% (2790/2790), done. Writing objects: 100% (2842/2842), 3.07 MiB | 5.54 MiB/s, done. Total 2842 (delta 2204), reused 0 (delta 0) Patting myself on the back and stopping mid-way to my back, I realized that both GitHub and BitBucket do not really support repos bigger than 1GB and I was sitting with 26GB :) All the art assets had been committed as part of the code repository and I did not know which were needed and which weren\u0026rsquo;t. The other devs / designers on the team were not familiar with Git and we decided to rather stick with SVN on a micro AWS instance. Interestingly enough, the CPU on the micro instance affected checkout speed - when changed to a small instance, the checkout speed doubled.\n","date":"February 4, 2012","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2012-02-04/git-rpc-error/","summary":"While I was working on Dementium II HD, I wanted to migrate the repository from SVN to Git. It was being hosted on a machine in the one studio and served the external developers over a 10mbit ADSL connection. The upstream rate is ~400kbit, so imagine trying to checkout a 26GB repository. I ran into the following error when I tried to import the SVN repo into a Git one:","tags":["git","svn"],"title":"Migrating repo from SVN to Git RPC error"},{"categories":["linux"],"contents":"Getting Ubuntu to run on Hyper-V was a bit of an issue for Windows Server 2008, here are some things to look for. In the boot menu, add vga16fb.modeset=0 to the boot options by hitting F6 to disable framebuffer mode that is really slow under Hyper-V.\nDo normal install, don’t worry about the red “can’t find network issue”.\nEnable the Hyper-V modules:\necho \u0026#34;hv_vmbus\u0026#34; \u0026gt;\u0026gt; /etc/initramfs-tools/modules echo \u0026#34;hv_storvsc\u0026#34; \u0026gt;\u0026gt; /etc/initramfs-tools/modules echo \u0026#34;hv_blkvsc\u0026#34; \u0026gt;\u0026gt; /etc/initramfs-tools/modules echo \u0026#34;hv_netvsc\u0026#34; \u0026gt;\u0026gt; /etc/initramfs-tools/modules update-initramfs –u Disable Framebuffer (otherwise the text screen scrolls by like a 9600 modem)\necho \u0026#34;blacklist vga16fb\u0026#34; \u0026gt;\u0026gt; /etc/modprobe.d/blacklist-framebuffer.conf Add networking config to /etc/network/interfaces:\nauto eth0 iface eth0 inet loopback Static: Auto eth0 iface eth0 inet static address [insert your IP address] netmask [insert your netmask] Gateway [insert your gateway address] If you see something about /dev/sdaX not begin writable during the apt-get upgrade process, do this:\nNB!!!! Make sure that power management is set to Maximum performance, otherwise you get a 18560 “triple fault” error.\nUpdate: This is not entirely Hyper-V’s fault, its the way the grub update goes. Do this:\napt-get update apt-get install -u grub Then edit /etc/default/grub:\nGRUB_CMDLINE_LINUX=\u0026#34;ide0=noprobe ide1=noprobe hda=noprobe hdb=noprobe\u0026#34; GRUB_DISABLE_LINUX_UUID=true And run:\nupdate-grub Change fstab from UUID to /dev/sdaX.\nREBOOT!!! The upgrade your system again.\napt-get upgrade Read here for more info:\nhttp://ubuntuforums.org/archive/index.php/t-1641951.html http://blogs.msdn.com/b/virtual_pc_guy/archive/2010/10/21/installing-ubuntu-server-10-10-on-hyper-v.aspx ","date":"July 25, 2011","hero":"/images/default-hero.jpg","permalink":"http://localhost:1313/posts/2011-07-25/ubuntu-on-hyper-v/","summary":"Getting Ubuntu to run on Hyper-V was a bit of an issue for Windows Server 2008, here are some things to look for. In the boot menu, add vga16fb.modeset=0 to the boot options by hitting F6 to disable framebuffer mode that is really slow under Hyper-V.\nDo normal install, don’t worry about the red “can’t find network issue”.\nEnable the Hyper-V modules:\necho \u0026#34;hv_vmbus\u0026#34; \u0026gt;\u0026gt; /etc/initramfs-tools/modules echo \u0026#34;hv_storvsc\u0026#34; \u0026gt;\u0026gt; /etc/initramfs-tools/modules echo \u0026#34;hv_blkvsc\u0026#34; \u0026gt;\u0026gt; /etc/initramfs-tools/modules echo \u0026#34;hv_netvsc\u0026#34; \u0026gt;\u0026gt; /etc/initramfs-tools/modules update-initramfs –u Disable Framebuffer (otherwise the text screen scrolls by like a 9600 modem)","tags":["linux","ubuntu","hyper-v"],"title":"Ubuntu on Hyper-V"}]